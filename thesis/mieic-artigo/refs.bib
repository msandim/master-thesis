Automatically generated by Mendeley Desktop 1.17.10
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@incollection{Re2011,
abstract = {Ensemble methods are statistical and computational learning procedures reminiscent of the human social learning behaviour of seeking several opinions before making any crucial decision. The idea of combining the opinions of different” experts” to obtain an overall” ...},
author = {Re, Matteo and Valentini, Giorgio},
booktitle = {Data Mining and Machine Learning for Astronomical Applications},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Re, Valentini - Unknown - Chapter 1 Ensemble methods a review.pdf:pdf},
isbn = {0849300525},
pages = {1--40},
title = {{Ensemble methods: a review}},
year = {2011}
}
@article{similaritymeasures,
abstract = {The binary feature vector is one of the most common representations of patterns and measuring similarity and distance measures play a critical role in many problems such as clustering, classification, etc. Ever since Jaccard proposed a similarity measure to classify ecological species in 1901, numerous binary similarity and distance measures have been proposed in various fields. Applying appropriate measures results in more accurate data analysis. Notwithstanding, few comprehensive surveys on binary measures have been conducted. Hence we collected 76 binary similarity and distance measures used over the last century and reveal their correlations through the hierarchical clustering technique. [ABSTRACT FROM AUTHOR]},
author = {Seung-Seok, Choi and Sung-Hyuk, Cha and Tappert, Charles C},
doi = {10.1.1.352.6123},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Seung-Seok, Sung-Hyuk, Tappert - 2010 - A Survey of Binary Similarity and Distance Measures.pdf:pdf},
isbn = {1934272612},
issn = {16904524},
journal = {Journal of Systemics, Cybernetics {\&} Informatics},
keywords = {BINARY control systems,BINARY system (Mathematics),BINARY-coded decimal system,BINOMIAL coefficients,Binary,CLUSTER analysis (Statistics),COMPUTER arithmetic,COMPUTER science,FORMS,binary distance measure,binary similarity measure,classification,hierarchical clustering,operational taxonomic unit},
number = {1},
pages = {43--48},
title = {{A Survey of Binary Similarity and Distance Measures.}},
url = {http://www.iiisci.org/journal/CV{\$}/sci/pdfs/GS315JG.pdf http://ezproxy.uthm.edu.my/login?url=http://search.ebscohost.com/login.aspx?direct=true{\&}db=aph{\&}AN=59856128{\&}site=ehost-live{\&}scope=site},
volume = {8},
year = {2010}
}
@inproceedings{Barbara:2003:BDM:952532.952616,
address = {New York, NY, USA},
author = {Barbar{\'{a}}, Daniel and Li, Yi and Couto, Julia and Lin, Jia-Ling and Jajodia, Sushil},
booktitle = {Proceedings of the 2003 ACM Symposium on Applied Computing},
doi = {10.1145/952532.952616},
isbn = {1-58113-624-2},
keywords = {clustering,intrusion detection,outliers},
pages = {421--425},
publisher = {ACM},
series = {SAC '03},
title = {{Bootstrapping a Data Mining Intrusion Detection System}},
url = {http://doi.acm.org/10.1145/952532.952616},
year = {2003}
}
@article{hartigan1979algorithm,
author = {Hartigan, John A and Wong, Manchek A},
journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
number = {1},
pages = {100--108},
publisher = {JSTOR},
title = {{Algorithm AS 136: A k-means clustering algorithm}},
volume = {28},
year = {1979}
}
@inproceedings{Nguyen2010,
abstract = {Outlier detection has many practical applications, especially in domains that have scope for abnormal behavior. Despite the importance of detecting outliers, defining outliers in fact is a nontrivial task which is normally application-dependent. On the other hand, detection techniques are constructed around the chosen definitions. As a consequence, available detection techniques vary significantly in terms of accuracy, performance and issues of the detection problem which they address. In this paper, we propose a unified framework for combining different outlier detection algorithms. Unlike existing work, our approach combines non-compatible techniques of different types to improve the outlier detection accuracy compared to other ensemble and individual approaches. Through extensive empirical studies, our framework is shown to be very effective in detecting outliers in the real-world context. {\textcopyright} Springer-Verlag Berlin Heidelberg 2010.},
author = {Nguyen, Hoang Vu and Ang, Hock Hee and Gopalkrishnan, Vivekanand},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-12026-8_29},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen, Ang, Gopalkrishnan - 2010 - Mining outliers with ensemble of heterogeneous detectors on random subspaces.pdf:pdf},
isbn = {3642120253},
issn = {03029743},
number = {PART 1},
pages = {368--383},
publisher = {Springer, Berlin, Heidelberg},
title = {{Mining outliers with ensemble of heterogeneous detectors on random subspaces}},
url = {http://link.springer.com/10.1007/978-3-642-12026-8{\_}29},
volume = {5981 LNCS},
year = {2010}
}
@article{Khreich2011,
abstract = {Hidden Markov models (HMMs) have been successfully applied in many intrusion detection applications, including anomaly detection from sequences of operating system calls. In practice, anomaly detection systems (ADSs) based on HMMs typically generate false alarms because they are designed using limited amount of representative training data. Since new data may become available over time, an important feature of an ADS is the ability to accommodate newly acquired data incrementally, after it has originally been trained and deployed for operations. In this paper, a system based on the receiver operating characteristic (ROC) is proposed to efficiently adapt ensembles of HMMs (EoHMMs) in response to new data, according to a learn-and-combine approach. When a new block of training data becomes available, a pool of base HMMs is generated from the data using a different number of HMM states and random initializations. The responses from the newly trained HMMs are then combined to those of the previously trained HMMs in ROC space using a novel incremental Boolean combination (incrBC) technique. Finally, specialized algorithms for model management allow to select a diversified EoHMM from the pool, and adapt Boolean fusion functions and thresholds for improved performance, while it prunes redundant base HMMs. The proposed system is capable of changing the desired operating point during operations, and this point can be adjusted to changes in prior probabilities and costs of errors. Computer simulations conducted on synthetic and real-world host-based intrusion detection data indicate that the proposed system can achieve a significantly higher level of performance than when parameters of a single best HMM are estimated, at each learning stage, using reference batch and incremental learning techniques. It also outperforms the learn-and-combine approaches using static fusion functions (e.g., majority voting). Over time, the proposed ensemble selection algorithms form compact EoHMMs, while maintaining or improving system accuracy. Pruning allows to limit the pool size from increasing indefinitely, thereby reducing the storage space for accommodating HMMs parameters without negatively affecting the overall EoHMM performance. Although applied for HMM-based ADSs, the proposed approach is general and can be employed for a wide range of classifiers and detection applications. ?? 2011 Elsevier Ltd. All rights reserved.},
author = {Khreich, Wael and Granger, Eric and Miri, Ali and Sabourin, Robert},
doi = {10.1016/j.patcog.2011.06.014},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Khreich et al. - 2012 - Adaptive ROC-based ensembles of HMMs applied to anomaly detection(2).pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Adaptive systems,Anomaly detection,Classification,Computer and network security,Hidden Markov models,Incremental learning,Information fusion,Multi-classifier systems,ROC},
number = {1},
pages = {208--230},
title = {{Adaptive ROC-based ensembles of HMMs applied to anomaly detection}},
volume = {45},
year = {2012}
}
@article{Goldstein2016,
abstract = {Anomaly detection is the process of identifying unexpected items or events in datasets, which differ from the norm. In contrast to standard classification tasks, anomaly detection is often applied on unlabeled data, taking only the internal structure of the dataset into account. This challenge is known as unsupervised anomaly detection and is addressed in many practical applications, for example in network intrusion detection, fraud detection as well as in the life science and medical domain. Dozens of algorithms have been proposed in this area, but unfortunately the research community still lacks a comparative universal evaluation as well as common publicly available datasets. These shortcomings are addressed in this study, where 19 different unsupervised anomaly detection algorithms are evaluated on 10 different datasets from multiple application domains. By publishing the source code and the datasets, this paper aims to be a new well-funded basis for unsupervised anomaly detection research. Additionally, this evaluation reveals the strengths and weaknesses of the different approaches for the first time. Besides the anomaly detection performance, computational effort, the impact of parameter settings as well as the global/local anomaly detection behavior is outlined. As a conclusion, we give an advise on algorithm selection for typical real-world tasks.},
author = {Goldstein, Markus and Uchida, Seiichi},
doi = {10.1371/journal.pone.0152173},
editor = {Zhu, Dongxiao},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/journal.pone.0152173.PDF:PDF},
issn = {19326203},
journal = {PLoS ONE},
month = {apr},
number = {4},
pages = {e0152173},
pmid = {27093601},
publisher = {Public Library of Science},
title = {{A comparative evaluation of unsupervised anomaly detection algorithms for multivariate data}},
url = {http://dx.plos.org/10.1371/journal.pone.0152173},
volume = {11},
year = {2016}
}
@inproceedings{Kriegel,
abstract = {Outlier scores provided by different outlier models differ widely in their meaning, range, and contrast between different outlier models and, hence, are not easily comparable or interpretable. We propose a unification of outlier scores provided by various outlier models and a translation of the arbitrary “outlier factors” to values in the range [0, 1] interpretable as values describing the probability of a data object of being an outlier. As an application, we show that this unification facilitates enhanced ensembles for outlier detection.},
annote = {NULL},
author = {Kriegel, Hans-Peter and Kroeger, Peer and Schubert, Erich and Zimek, Arthur},
booktitle = {Proceedings of the 2011 SIAM International Conference on Data Mining},
doi = {http://dx.doi.org/10.1137/1.9781611972818.2},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kriegel et al. - Unknown - Interpreting and Unifying Outlier Scores.pdf:pdf},
isbn = {978-1-61197-262-7},
pages = {13--24},
title = {{Interpreting and Unifying Outlier Scores}},
url = {http://www.dbs.ifi.lmu.de http://epubs.siam.org/doi/abs/10.1137/1.9781611972818.2},
year = {2011}
}
@article{Noto,
abstract = {We present a new approach to semi-supervised anomaly detection. Given a set of training examples believed to come from the same distribution or class, the task is to learn a model that will be able to distinguish examples in the future that do not belong to the same class. Traditional approaches typically compare the position of a new data point to the set of "normal" training data points in a chosen representation of the feature space. For some data sets, the normal data may not have discernible positions in feature space, but do have consistent relationships among some features that fail to appear in the anomalous examples. Our approach learns to predict the values of training set features from the values of other features. After we have formed an ensemble of predictors, we apply this ensemble to new data points. To combine the contribution of each predictor in our ensemble, we have developed a novel, information-theoretic anomaly measure that our experimental results show selects against noisy and irrelevant features. Our results on 47 data sets show that for most data sets, this approach significantly improves performance over current state-of-the-art feature space distance and density-based approaches.},
author = {Noto, Keith and Brodley, Carla and Slonim, Donna},
doi = {10.1109/ICDM.2010.140},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Noto, Brodley, Slonim - 2010 - Anomaly Detection Using an Ensemble of Feature Models.pdf:pdf},
isbn = {978-1-4244-9131-5},
issn = {1550-4786},
journal = {Proceedings / IEEE International Conference on Data Mining. IEEE International Conference on Data Mining},
keywords = {-anomaly detection,feature se-,machine learning},
pages = {953--958},
pmid = {22020249},
title = {{Anomaly Detection Using an Ensemble of Feature Models}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3197694{\&}tool=pmcentrez{\&}rendertype=abstract},
year = {2010}
}
@article{Oza2008,
abstract = {Broad classes of statistical classification algorithms have been developed and applied successfully to a wide range of real-world domains. In general, ensuring that the particular classification algorithm matches the properties of the data is crucial in providing results that meet the needs of the particular application domain. One way in which the impact of this algorithm/application match can be alleviated is by using ensembles of classifiers, where a variety of classifiers (either different types of classifiers or different instantiations of the same classifier) are pooled before a final classification decision is made. Intuitively, classifier ensembles allow the different needs of a difficult problem to be handled by classifiers suited to those particular needs. Mathematically, classifier ensembles provide an extra degree of freedom in the classical bias/variance tradeoff, allowing solutions that would be difficult (if not impossible) to reach with only a single classifier. Because of these advantages, classifier ensembles have been applied to many difficult real-world problems. In this paper, we survey select applications of ensemble methods to problems that have historically been most representative of the difficulties in classification. In particular, we survey applications of ensemble methods to remote sensing, person recognition, one vs. all recognition, and medicine. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Oza, Nikunj C. and Tumer, Kagan},
doi = {10.1016/j.inffus.2007.07.002},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/oza2008.pdf:pdf},
isbn = {1566-2535},
issn = {15662535},
journal = {Information Fusion},
keywords = {Classifier ensembles,Ensemble applications},
month = {jan},
number = {1},
pages = {4--20},
title = {{Classifier ensembles: Select real-world applications}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1566253507000620},
volume = {9},
year = {2008}
}
@article{Breunig:2000:LID:335191.335388,
address = {New York, NY, USA},
author = {Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, J{\"{o}}rg},
doi = {10.1145/335191.335388},
issn = {0163-5808},
journal = {SIGMOD Rec.},
keywords = {database mining,outlier detection},
month = {may},
number = {2},
pages = {93--104},
publisher = {ACM},
title = {{LOF: Identifying Density-based Local Outliers}},
url = {http://doi.acm.org/10.1145/335191.335388},
volume = {29},
year = {2000}
}
@article{He:2003:DCL:770340.770389,
address = {New York, NY, USA},
author = {He, Zengyou and Xu, Xiaofei and Deng, Shengchun},
doi = {10.1016/S0167-8655(03)00003-5},
issn = {0167-8655},
journal = {Pattern Recogn. Lett.},
keywords = {clustering,data mining,outlier detection},
number = {9-10},
pages = {1641--1650},
publisher = {Elsevier Science Inc.},
title = {{Discovering Cluster-based Local Outliers}},
volume = {24},
year = {2003}
}
@article{Zimek2014a,
abstract = {Outlier detection and ensemble learning are well established re-search directions in data mining yet the application of ensemble techniques to outlier detection has been rarely studied. Building an ensemble requires learning of diverse models and combining these diverse models in an appropriate way. We propose data perturba-tion as a new technique to induce diversity in individual outlier de-tectors as well as a rank accumulation method for the combination of the individual outlier rankings in order to construct an outlier detection ensemble. In an extensive evaluation, we study the im-pact, potential, and shortcomings of this new approach for outlier detection ensembles. We show that this ensemble can significantly improve over weak performing base methods.},
address = {New York, New York, USA},
annote = {NULL},
author = {Zimek, Arthur and Campello, Ricardo J. G. B. and Sander, J{\"{o}}rg},
doi = {10.1145/2618243.2618257},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/zimek2014.pdf:pdf},
isbn = {9781450327220},
journal = {Proceedings of the 26th International Conference on Scientific and Statistical Database Management - SSDBM '14},
keywords = {ensemble,outlier detection},
pages = {1--12},
publisher = {ACM Press},
title = {{Data perturbation for outlier detection ensembles}},
url = {http://dl.acm.org/citation.cfm?doid=2618243.2618257},
year = {2014}
}
@article{sigillito1989classification,
author = {Sigillito, Vincent G and Wing, Simon P and Hutton, Larrie V and Baker, Kile B},
journal = {Johns Hopkins APL Technical Digest},
number = {3},
pages = {262--266},
title = {{Classification of radar returns from the ionosphere using neural networks}},
volume = {10},
year = {1989}
}
@article{708428,
author = {Hearst, M A and Dumais, S T and Osuna, E and Platt, J and Scholkopf, B},
doi = {10.1109/5254.708428},
issn = {1094-7167},
journal = {IEEE Intelligent Systems and their Applications},
keywords = {computational linguistics;face recognition;learnin},
month = {jul},
number = {4},
pages = {18--28},
title = {{Support vector machines}},
volume = {13},
year = {1998}
}
@article{Breunig,
abstract = {For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in out- lier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. This degree is called the local outlier factor (LOF) of an object. It is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using real- world datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding lo- cal outliers can be practical.},
author = {Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, J{\"{o}}rg},
doi = {10.1145/335191.335388},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breunig et al. - 2000 - LOF Identifying Density-Based Local Outliers.pdf:pdf},
isbn = {1581132182},
issn = {01635808},
journal = {Proceedings of the 2000 Acm Sigmod International Conference on Management of Data},
keywords = {database mining,outlier detection},
pages = {1--12},
title = {{LOF: Identifying Density-Based Local Outliers}},
year = {2000}
}
@article{Zimek2014,
abstract = {Ensembles for unsupervised outlier detection is an emerging topic that has been neglected for a surprisingly long time (although there are reasons why this is more difficult than supervised ensembles or even clustering ensembles). Aggarwal recently discussed algorithmic patterns of outlier detection ensembles, identified traces of the idea in the literature, and remarked on potential as well as unlikely avenues for future transfer of concepts from supervised ensembles. Complementary to his points, here we focus on the core ingredients for building an outlier ensemble, discuss the first steps taken in the literature, and identify challenges for future research.},
author = {Zimek, Arthur and Campello, Ricardo J G B and Sander, J{\"{o}}rg},
doi = {10.1145/2594473.2594476},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zimek, Campello - Unknown - Ensembles for Unsupervised Outlier Detection Challenges and Research Questions.pdf:pdf},
isbn = {1931-0145},
issn = {19310145},
journal = {ACM SIGKDD Explorations Newsletter},
number = {1},
pages = {11--22},
title = {{Ensembles for unsupervised outlier detection: Challenges and research questions}},
volume = {15},
year = {2014}
}
@article{Sesmero2015,
abstract = {Over the last two decades, the machine learning and related communities have conducted numerous studies to improve the performance of a single classifier by combining several classifiers generated from one or more learning algorithms. Bagging and Boosting are the most representative examples of algorithms for generating homogeneous ensembles of classifiers. However, Stacking has become a commonly used technique for generating ensembles of heterogeneous classifiers since Wolpert presented his study entitled Stacked Generalization in 1992. Studies that have addressed the Stacking issue demonstrated that when selecting base learning algorithms for generating classifiers that are members of the ensemble, their learning parameters and the learning algorithm for generating the meta-classifier were critical issues. Most studies on this topic manually select the appropriate combination of base learning algorithms and their learning parameters. However, some other methods use automatic methods to determine good Stacking configurations instead of starting from these strong initial assumptions. In this paper, we describe Stacking and its variants and present several examples of application domains.},
author = {Sesmero, M. Paz and Ledezma, Agapito I. and Sanchis, Araceli},
doi = {10.1002/widm.1143},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sesmero, Ledezma, Sanchis - 2015 - Generating ensembles of heterogeneous classifiers using Stacked Generalization.pdf:pdf},
issn = {19424795},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
month = {jan},
number = {1},
pages = {21--34},
title = {{Generating ensembles of heterogeneous classifiers using Stacked Generalization}},
url = {http://doi.wiley.com/10.1002/widm.1143},
volume = {5},
year = {2015}
}
@article{Lazarevic2005,
abstract = {Outlier detection has recently become an important problem in many industrial and financial applications. In this paper, a novel feature bagging approach for detecting outliers in very large, high dimensional and noisy databases is proposed. It combines results from multiple outlier detection algorithms that are applied using different set of features. Every outlier detection algorithm uses a small subset of features that are randomly selected from the original feature set. As a result, each outlier detector identifies different outliers, and thus assigns to all data records outlier scores that correspond to their probability of being outliers. The outlier scores computed by the individual outlier detection algorithms are then combined in order to find the better quality outliers. Experiments performed on several synthetic and real life data sets show that the proposed methods for combining outputs from multiple outlier detection algorithms provide non-trivial improvements over the base algorithm.},
address = {New York, New York, USA},
author = {Lazarevic, Aleksandar and Kumar, Vipin},
doi = {10.1145/1081870.1081891},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/10.1.1.399.425.pdf:pdf},
isbn = {159593135X},
journal = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
keywords = {bagging,detection,feature subsets,integration,outlier detection},
pages = {157--166},
publisher = {ACM Press},
title = {{Feature bagging for outlier detection}},
url = {http://portal.acm.org/citation.cfm?doid=1081870.1081891 http://dl.acm.org/citation.cfm?id=1081891},
year = {2005}
}
@article{Read2011,
abstract = {The widely known binary relevance method for multi-label classification, which considers each label as an independent binary problem, has often been overlooked in the literature due to the perceived inadequacy of not directly modelling label correlations. Most current methods invest considerable complexity to model interdependencies between labels. This paper shows that binary relevance-based methods have much to offer, and that high predictive performance can be obtained without impeding scalability to large datasets. We exemplify this with a novel classifier chains method that can model label correlations while maintaining acceptable computational complexity. We extend this approach further in an ensemble framework. An extensive empirical evaluation covers a broad range of multi-label datasets with a variety of evaluation metrics. The results illustrate the competitiveness of the chaining method against related and state-of-the-art methods, both in terms of predictive performance and time complexity.},
archivePrefix = {arXiv},
arxivId = {arXiv:1207.6324},
author = {Read, Jesse and Pfahringer, Bernhard and Holmes, Geoff and Frank, Eibe},
doi = {10.1007/s10994-011-5256-5},
eprint = {arXiv:1207.6324},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read et al. - 2011 - Classifier chains for multi-label classification.pdf:pdf},
isbn = {9783642041730},
issn = {08856125},
journal = {Machine Learning},
keywords = {Ensemble methods,Multi-label classification,Problem transformation,Scalable methods},
number = {3},
pages = {333--359},
pmid = {22183238},
title = {{Classifier chains for multi-label classification}},
volume = {85},
year = {2011}
}
@article{liaw2002classification,
author = {Liaw, Andy and Wiener, Matthew and Others},
journal = {R news},
number = {3},
pages = {18--22},
title = {{Classification and regression by randomForest}},
volume = {2},
year = {2002}
}
@article{Tang2002,
abstract = {Outlier detection is concerned with discovering exceptional behaviors of objects in data sets.It is becoming a growingly useful tool in applications such as credit card fraud detection, discovering criminal behaviors in e-commerce, identifying computer intrusion, detecting health problems, etc.In this paper, we introduce a connectivity-based outlier factor (COF) scheme that improves the effectiveness of an existing local outlier factor (LOF) scheme when a pattern itself has similar neighbourhood density as an outlier.We give theoretical and empirical analysis to demonstrate the improvement in effectiveness and the capability of the COF scheme in comparison with the LOF scheme.},
author = {Tang, Jian and Chen, Zhixiang and Fu, Ada Wai-chee and Cheung, David W.},
doi = {10.1007/3-540-47887-6},
file = {::},
isbn = {3-540-43704-5},
issn = {16113349},
journal = {Advances in Knowledge Discovery and Data Mining},
pages = {535--548},
pmid = {25246403},
publisher = {Springer Berlin Heidelberg},
title = {{Enhancing effectiveness of outlier detections for low density patterns}},
volume = {2336},
year = {2002}
}
@incollection{Polikar2012a,
address = {Boston, MA},
author = {Polikar, Robi},
booktitle = {Ensemble Machine Learning},
doi = {10.1007/978-1-4419-9326-7_1},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Polikar - 2012 - Ensemble learning(2).pdf:pdf},
isbn = {1441993258},
pages = {1--34},
publisher = {Springer US},
title = {{Ensemble learning}},
url = {http://link.springer.com/10.1007/978-1-4419-9326-7{\_}1},
year = {2012}
}
@inproceedings{He2005,
abstract = {The task of outlier detection is to find small groups of data objects that are exceptional when compared with rest large amount of data. Detection of such outliers is important for many applications such as fraud detection and customer migration. Most such applications are high dimensional domains in which the data may contain hundreds of dimensions. However, the outlier detection problem itself is not well defined and none of the existing definitions are widely accepted, especially in high dimensional space. In this paper, our first contribution is to propose a unified framework for outlier detection in high dimensional spaces from an ensemble-learning viewpoint. In our new framework, the outlying-ness of each data object is measured by fusing outlier factors in different subspaces using a combination function. Accordingly, we show that all existing researches on outlier detection can be regarded as special cases in the unified framework with respect to the set of subspaces considered and the type of combination function used. In addition, to demonstrate the usefulness of the ensemble-learning based outlier detection framework, we developed a very simple and fast algorithm, namely SOE1 (Subspace Outlier Ensemble using 1-dimensional Subspaces) in which only subspaces with one dimension is used for mining outliers from large categorical datasets. The SOE1 algorithm needs only two scans over the dataset and hence is very appealing in real data mining applications. Experimental results on real datasets and large synthetic datasets show that: (1) SOE1 has comparable performance with respect to those state-of-art outlier detection algorithms on identifying true outliers and (2) SOE1 can be an order of magnitude faster than one of the fastest outlier detection algorithms known so far.},
archivePrefix = {arXiv},
arxivId = {cs/0505060},
author = {He, Zengyou and Deng, Shengchun and Xu, Xiaofei},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/11563952_56},
eprint = {0505060},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/He, Deng, Xu - 2005 - A unified subspace outlier ensemble framework for outlier detection.pdf:pdf},
isbn = {3540292276},
issn = {03029743},
pages = {632--637},
primaryClass = {cs},
publisher = {Springer, Berlin, Heidelberg},
title = {{A unified subspace outlier ensemble framework for outlier detection}},
url = {http://link.springer.com/10.1007/11563952{\_}56},
volume = {3739 LNCS},
year = {2005}
}
@article{lasi2014industry,
author = {Lasi, Heiner and Fettke, Peter and Kemper, Hans-Georg and Feld, Thomas and Hoffmann, Michael},
journal = {Business {\&} Information Systems Engineering},
number = {4},
pages = {239},
publisher = {Springer Science {\&} Business Media},
title = {{Industry 4.0}},
volume = {6},
year = {2014}
}
@inproceedings{Perdisci2006,
abstract = {Unsupervised or unlabeled learning approaches for network anomaly detection have been recently proposed. In particular, recent work on unlabeled anomaly detection focused on high speed classification based on simple payload statistics. For example, PAYL, an anomaly IDS, measures the occurrence frequency in the payload of n-grams. A simple model of normal traffic is then constructed according to this description of the packets' content. It has been demonstrated that anomaly detectors based on payload statistics can be "evaded" by mimicry attacks using byte substitution and padding techniques. In this paper we propose a new approach to construct high speed payload-based anomaly IDS intended to be accurate and hard to evade. We propose a new technique to extract the features from the payload. We use a feature clustering algorithm originally proposed for text classification problems to reduce the dimensionality of the feature space. Accuracy and hardness of evasion are obtained by constructing our anomaly-based IDS using an ensemble of one-class SVM classifiers that work on different feature spaces.},
author = {Perdisci, Roberto and Gu, Ofei and Lee, Wenke},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.1109/ICDM.2006.165},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/04053075.pdf:pdf},
isbn = {0769527019},
issn = {15504786},
month = {dec},
pages = {488--498},
publisher = {IEEE},
title = {{Using an ensemble of one-class SVM classifiers to harden payload-based anomaly detection systems}},
url = {http://ieeexplore.ieee.org/document/4053075/},
year = {2006}
}
@article{Liu2012,
abstract = {Anomalies are data points that are few and different. As a result of these properties, we show that, anomalies are susceptible to a mechanism called isolation. This article proposes a method called Isolation Forest (iForest), which detects anomalies purely based on the concept of isolation without employing any distance or density measure---fundamentally different from all existing methods. As a result, iForest is able to exploit subsampling (i) to achieve a low linear time-complexity and a small memory-requirement and (ii) to deal with the effects of swamping and masking effectively. Our empirical evaluation shows that iForest outperforms ORCA, one-class SVM, LOF and Random Forests in terms of AUC, processing time, and it is robust against masking and swamping effects. iForest also works well in high dimensional problems containing a large number of irrelevant attributes, and when anomalies are not available in training sample.},
author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
doi = {10.1145/2133360.2133363},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/tkdd11.pdf:pdf},
isbn = {1556-4681},
issn = {15564681},
journal = {ACM Transactions on Knowledge Discovery from Data},
keywords = {ACM,Anomaly detection,binary tree,ensemble methods,isolation,isolation forest,outlier detection,random tree ensemble},
month = {mar},
number = {1},
pages = {1--39},
publisher = {ACM},
title = {{Isolation-Based Anomaly Detection}},
url = {http://dl.acm.org/citation.cfm?doid=2133360.2133363},
volume = {6},
year = {2012}
}
@book{breiman1984classification,
author = {Breiman, Leo and Friedman, Jerome and Stone, Charles J and Olshen, Richard A},
publisher = {CRC press},
title = {{Classification and regression trees}},
year = {1984}
}
@inproceedings{mccallum1998comparison,
author = {McCallum, Andrew and Nigam, Kamal and Others},
booktitle = {AAAI-98 workshop on learning for text categorization},
organization = {Madison, WI},
pages = {41--48},
title = {{A comparison of event models for naive bayes text classification}},
volume = {752},
year = {1998}
}
@article{Pasillas-Diaz:2016:UAC:3032128.3032199,
address = {Amsterdam, The Netherlands, The Netherlands},
author = {Pasillas-D$\backslash$'$\backslash$iaz, Jos{\'{e}} Ram{\'{o}}n and Ratt{\'{e}}, Sylvie},
doi = {10.1016/j.entcs.2016.12.005},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/1-s2.0-S1571066116301128-main.pdf:pdf},
issn = {1571-0661},
journal = {Electron. Notes Theor. Comput. Sci.},
keywords = {ensembles,outlier detection},
month = {dec},
number = {C},
pages = {61--77},
publisher = {Elsevier Science Publishers B. V.},
title = {{An Unsupervised Approach for Combining Scores of Outlier Detection Techniques, Based on Similarity Measures}},
url = {https://doi.org/10.1016/j.entcs.2016.12.005},
volume = {329},
year = {2016}
}
@article{powers2011evaluation,
author = {Powers, David Martin},
publisher = {Bioinfo Publications},
title = {{Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation}},
year = {2011}
}
@inproceedings{Alimoglu96methodsof,
author = {Alimoglu, Fevzi and Alpaydin, Ethem},
booktitle = {Proceedings of the Fifth Turkish Artificial Intelligence and Artificial Neural Networks Symposium (TAINN 96},
title = {{Methods of Combining Multiple Classifiers Based on Different Representations for Pen-based Handwritten Digit Recognition}},
year = {1996}
}
@book{Kuncheva2004,
abstract = {"A Wiley-Interscience publication." 1. Fundamentals of pattern recognition -- 2. Base classifiers -- 3. Multiple classifier systems -- 4. Fusion of label outputs -- 5. Fusion of continuous-valued outputs -- 6. Classifier selection -- 7. Bagging and boosting -- 8. Miscellanea -- 9. Theoretical views and results -- 10. Diversity in classifier ensembles.},
author = {Kuncheva, Ludmila I. (Ludmila Ilieva) and I., Ludmila},
file = {::},
isbn = {0471210781},
pages = {350},
title = {{Combining pattern classifiers : methods and algorithms}},
url = {https://pdfs.semanticscholar.org/453c/2b407c57d7512fdbe19fa1cefa08dd22614a.pdf},
year = {2004}
}
@inproceedings{Ester:1996:DAD:3001460.3001507,
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"{o}}rg and Xu, Xiaowei},
booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
keywords = {arbitrary shape of clusters,clustering algorithms,efficiency on large spatial databases,handling nlj4-275oise},
pages = {226--231},
publisher = {AAAI Press},
series = {KDD'96},
title = {{A Density-based Algorithm for Discovering Clusters a Density-based Algorithm for Discovering Clusters in Large Spatial Databases with Noise}},
year = {1996}
}
@article{Patcha2007a,
abstract = {As advances in networking technology help to connect the distant corners of the globe and as the Internet continues to expand its influence as a medium for communications and commerce, the threat from spammers, attackers and criminal enterprises has also grown accordingly. It is the prevalence of such threats that has made intrusion detection systems-the cyberspace's equivalent to the burglar alarm-join ranks with firewalls as one of the fundamental technologies for network security. However, today's commercially available intrusion detection systems are predominantly signature-based intrusion detection systems that are designed to detect known attacks by utilizing the signatures of those attacks. Such systems require frequent rule-base updates and signature updates, and are not capable of detecting unknown attacks. In contrast, anomaly detection systems, a subset of intrusion detection systems, model the normal system/network behavior which enables them to be extremely effective in finding and foiling both known as well as unknown or "zero day" attacks. While anomaly detection systems are attractive conceptually, a host of technological problems need to be overcome before they can be widely adopted. These problems include: high false alarm rate, failure to scale to gigabit speeds, etc. In this paper, we provide a comprehensive survey of anomaly detection systems and hybrid intrusion detection systems of the recent past and present. We also discuss recent technological trends in anomaly detection and identify open problems and challenges in this area. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Patcha, Animesh and Park, Jung Min},
doi = {10.1016/j.comnet.2007.02.001},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/10.1016@j.comnet.2007.02.001.pdf:pdf},
isbn = {1389-1286},
issn = {13891286},
journal = {Computer Networks},
keywords = {Anomaly detection,Data mining,Machine learning,Statistical anomaly detection,Survey},
month = {aug},
number = {12},
pages = {3448--3470},
title = {{An overview of anomaly detection techniques: Existing solutions and latest technological trends}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S138912860700062X},
volume = {51},
year = {2007}
}
@article{Boro2012,
address = {New York, New York, USA},
author = {Boro, Debojit and Nongpoh, Bernard and Bhattacharyya, Dhruba},
doi = {10.1145/2388576.2388596},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/boro2012.pdf:pdf},
isbn = {9781450316682},
journal = {Proceedings of the Fifth International Conference on Security of Information and Networks - SIN '12},
keywords = {bagging,boosting,ensemble,intrusion detection,stacking},
pages = {143--147},
publisher = {ACM Press},
title = {{Anomaly based intrusion detection using meta ensemble classifier}},
url = {http://dl.acm.org/citation.cfm?doid=2388576.2388596},
year = {2012}
}
@article{Hawkins,
abstract = {We consider the problem of finding outliers in large multi-variate databases. Outlier detection can be applied during the data cleans-ing process of data mining to identify problems with the data itself, and to fraud detection where groups of outliers are often of particular inter-est. We use replicator neural networks (RNNs) to provide a measure of the outlyingness of data records. The performance of the RNNs is as-sessed using a ranked score measure. The effectiveness of the RNNs for outlier detection is demonstrated on two publicly available databases.},
author = {Hawkins, Simon and He, Hongxing and Williams, Graham and Baxter, Rohan},
file = {::},
title = {{Outlier Detection Using Replicator Neural Networks}}
}
@inproceedings{laurikkala2000informal,
author = {Laurikkala, Jorma and Juhola, Martti and Kentala, Erna and Lavrac, N and Miksch, S and Kavsek, B},
booktitle = {Fifth International Workshop on Intelligent Data Analysis in Medicine and Pharmacology},
pages = {20--24},
title = {{Informal identification of outliers in medical data}},
volume = {1},
year = {2000}
}
@book{hawkins1980identification,
author = {Hawkins, Douglas M},
publisher = {Springer},
title = {{Identification of outliers}},
volume = {11},
year = {1980}
}
@article{Yu2016,
abstract = {Methods for unsupervised anomaly detection suffer from the fact that the data is unlabeled, making it difficult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classification and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classifier combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data.},
archivePrefix = {arXiv},
arxivId = {1610.07677},
author = {Yu, Edward and Parekh, Parth},
eprint = {1610.07677},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu, Parekh - 2016 - A Bayesian Ensemble for Unsupervised Anomaly Detection.pdf:pdf},
keywords = {ensemble methods,outlier detection},
title = {{A Bayesian Ensemble for Unsupervised Anomaly Detection}},
url = {http://arxiv.org/abs/1610.07677},
year = {2016}
}
@article{Jolliffe2002,
abstract = {Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. It is extremely versatile with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years.},
address = {New York},
author = {Jolliffe, I T},
doi = {10.1007/b98835},
isbn = {0-387-95442-2},
issn = {15364844},
journal = {Springer Series in Statistics},
pages = {487},
pmid = {21435900},
publisher = {Springer-Verlag},
series = {Springer Series in Statistics},
title = {{Principal Component Analysis, Second Edition}},
volume = {98},
year = {2002}
}
@article{hsu2003practical,
author = {Hsu, Chih-Wei and Chang, Chih-Chung and Lin, Chih-Jen and Others},
title = {{A practical guide to support vector classification}},
year = {2003}
}
@incollection{Bountouridis2016,
author = {Bountouridis, Dimitrios and Koops, Hendrik Vincent and Wiering, Frans and Veltkamp, Remco C.},
doi = {10.1007/978-3-319-46759-7_22},
file = {::},
pages = {286--300},
publisher = {Springer, Cham},
title = {{Music Outlier Detection Using Multiple Sequence Alignment and Independent Ensembles}},
url = {http://link.springer.com/10.1007/978-3-319-46759-7{\_}22},
year = {2016}
}
@article{Goix,
abstract = {When sufficient labeled data are available, clas-sical criteria based on Receiver Operating Char-acteristic (ROC) or Precision-Recall (PR) curves can be used to compare the performance of un-supervised anomaly detection algorithms. How-ever, in many situations, few or no data are la-beled. This calls for alternative criteria one can compute on non-labeled data. In this paper, two criteria that do not require labels are empirically shown to discriminate accurately (w.r.t. ROC or PR based criteria) between algorithms. These criteria are based on existing Excess-Mass (EM) and Mass-Volume (MV) curves, which generally cannot be well estimated in large dimension. A methodology based on feature sub-sampling and aggregating is also described and tested, extend-ing the use of these criteria to high-dimensional datasets and solving major drawbacks inherent to standard EM and MV curves.},
author = {Goix, Nicolas},
file = {::},
title = {{How to Evaluate the Quality of Unsupervised Anomaly Detection Algorithms?}}
}
@article{Strehl2002,
abstract = {This paper introduces the problem of combining multiple partitionings of a set of objects into a single consolidated clustering without accessing the features or algorithms that determined these partitionings. We rst identify several application scenarios for the resultant `knowledge reuse' framework that we call cluster ensembles. The cluster ensemble problem is then formalized as a combinatorial optimization problem in terms of shared mutual information. In addition to a direct maximization approach, we propose three e ective and efficient techniques for obtaining high-quality combiners (consensus functions). The rst combiner induces a similarity measure from the partitionings and then reclusters the objects. The second combiner is based on hypergraph partitioning. The third one collapses groups of clusters into meta-clusters which then compete for each object to determine the combined clustering. Due to the low computational costs of our techniques, it is quite feasible to use a supra-consensus function that evaluates all three approaches against the objective function and picks the best solution for a given situation. We evaluate the effectiveness of cluster ensembles in three qualitatively di erent application scenarios: (i) where the original clusters were formed based on non-identical sets of features, (ii) where the original clustering algorithms worked on non-identical sets of objects, and (iii) where a common data-set is used and the main purpose of combining multiple clusterings is to improve the quality and robustness of the solution. Promising results are obtained in all three situations for synthetic as well as real data-sets.},
author = {Strehl, Alexander and Ghosh, Joydeep},
doi = {10.1162/153244303321897735},
file = {::},
isbn = {0262511290},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {cluster analysis,clustering,clustering ensemble,consensus clustering,consensus functions,ensemble,knowledge reuse,multi-learner,mutual information,partitioning,systems,unsupervised learning},
pages = {583--617},
pmid = {21423337},
publisher = {JMLR.org},
title = {{Cluster Ensembles – A Knowledge Reuse Framework for Combining Multiple Partitions}},
url = {http://www.crossref.org/deleted{\_}DOI.html},
volume = {3},
year = {2002}
}
@article{Polikar2006,
abstract = {In matters of great importance that have financial, medical, social, or other implications, we often seek a second opinion before making a decision, sometimes a third, and sometimes many more. In doing so, we weigh the individual opinions, and combine them through some thought process to reach a final decision that is presumably the most informed one. The process of consulting "several experts" before making a final decision is perhaps second nature to us; yet, the extensive benefits of such a process in automated decision making applications have only recently been discovered by computational intelligence community. Also known under various other names, such as multiple classifier systems, committee of classifiers, or mixture of experts, ensemble based systems have shown to produce favorable results compared to those of single-expert systems for a broad range of applications and under a variety of scenarios. Design, implementation and application of such systems are the main topics of this article. Specifically, this paper reviews conditions under which ensemble based systems may be more beneficial than their single classifier counterparts, algorithms for generating individual components of the ensemble systems, and various procedures through which the individual classifiers can be combined. We discuss popular ensemble based algorithms, such as bagging, boosting, AdaBoost, stacked generalization, and hierarchical mixture of experts; as well as commonly used combination rules, including algebraic combination of outputs, voting based techniques, behavior knowledge space, and decision templates. Finally, we look at current and future research directions for novel applications of ensemble systems. Such applications include incremental learning, data fusion, feature selection, learning with missing features, confidence estimation, and error correcting output codes; all areas in which ensemble systems have shown great promise},
author = {Polikar, R.},
doi = {10.1109/MCAS.2006.1688199},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/polikar2006.pdf:pdf},
isbn = {1531-636X},
issn = {1531-636X},
journal = {Circuits and Systems Magazine, IEEE},
keywords = {AdaBoost,automated decision making,bagging,behavior knowledge space,boosting,confidence estimation,data fusion,decision making,decision support systems,decision templates,ensemble based systems,error correcting output codes,error correction codes,expert systems,feature selection,incremental learning,learning (artificial intelligence),learning systems,multiple classifier systems,pattern classification,reviews,sensor fusion,single-expert systems,stacked generalization,voting based techniques},
number = {3},
pages = {21--45},
title = {{Ensemble based systems in decision making}},
url = {http://ieeexplore.ieee.org/document/1688199/},
volume = {6},
year = {2006}
}
@article{Ting1999,
abstract = {Stacked generalization is a general method of using a high-level model to combine lower-level models to achieve greater predictive accuracy. In this paper we address two crucial issues which have been considered to be a `black art' in classification tasks ever since the introduction of stacked generalization in 1992 by Wolpert: the type of generalizer that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input. We find that best results are obtained when the higher-level model combines the confidence (and not just the predictions) of the lower-level ones. We demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms for classification tasks. We also compare the performance of stacked generalization with majority vote and published results of arcing and bagging.},
archivePrefix = {arXiv},
arxivId = {1105.5466},
author = {Ting, Kai Ming and Witten, Ian H},
doi = {10.1613/jair.594},
eprint = {1105.5466},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ting, Witten - 1999 - Issues in Stacked Generalization.pdf:pdf},
isbn = {1076-9757},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {271--289},
title = {{Issues in stacked generalization}},
url = {https://www.jair.org/media/594/live-594-1797-jair.pdf},
volume = {10},
year = {1999}
}
@inproceedings{Soudi2015,
abstract = {Anomaly detection systems rely on machine learning techniques to model the normal behavior of the system. This model is used during operation to detect anomalies due to attacks or design faults. Ensemble methods have been used to improve the overall detection accuracy by combining the outputs of several accurate and diverse models. Existing Boolean combination techniques either require an exponential number of combinations or sequential combinations that grow linearly with the number of iterations, which make them difficult to scale up and analyze. In this paper, we propose PBC (Pruning Boolean Combination), an efficient approach for selecting and combining anomaly detectors. PBC relies on two novel pruning techniques that we have developed to aggressively prune redundant and trivial detectors. Compared to existing work, PBC reduces significantly the number of detectors to combine, while keeping similar accuracy. We show the effectiveness of PBC when applying it to a large dataset.},
author = {Soudi, Amirreza and Khreich, Wael and Hamou-Lhadj, Abdelwahab},
booktitle = {Proceedings - 2015 IEEE International Conference on Software Quality, Reliability and Security, QRS 2015},
doi = {10.1109/QRS.2015.25},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/soudi2015.pdf:pdf},
isbn = {9781467379892},
keywords = {Anomaly Detection Systems,Boolean Combination,Intrusion Detection Systems,Multiple-Detector Systems,Pruning Techniques},
month = {aug},
pages = {109--118},
publisher = {IEEE},
title = {{An Anomaly Detection System Based on Ensemble of Detectors with Effective Pruning Techniques}},
url = {http://ieeexplore.ieee.org/document/7272921/},
year = {2015}
}
@article{Ando2015,
abstract = {The numerical, sequential observation of behaviors, such as trajectories, have become an important subject for data mining and knowledge discovery research. Processing the raw observation into representative features of the behaviors involves an implicit choice of time-scale and resolution, which critically affect the final output of the mining techniques. The choice is associated with the parameters of data-processing, e.g., smoothing and segmentation, which unintuitively yet strongly influence the intrinsic structure of the numerical data. Data mining techniques generally require users to provide an appropriately processed input, but selecting a resolution is an arduous task that may require an expensive, manual examination of outputs between different settings. In this paper, we propose a novel ensemble framework for aggregating outcomes in different settings of scale and resolution parameters for an anomaly detection task. Such a task is difficult for existing ensemble approaches based on weighted combination because: (a) evaluating and weighing an output requires training samples of anomalies which are generally unavailable, (b) the detectability of anomalies can depend on the resolution, i.e., the distinction from normal instances may only be apparent within a small, selective range of parameters. In the proposed framework, predictions based on different resolutions are aggregated to construct meta-feature representations of the behavior instances. The meta-features provide the discriminative information for conducting a clustering-based anomaly detection. In the proposed framework, two interrelated tasks of the behavior analysis: processing the numerical data and discovering anomalous patterns, are addressed jointly, providing an intuitive alternative for a knowledge-intensive parameter selection. We also design an efficient clustering-based anomaly detection algorithm which reduces the computational burden of mining at multiple resolutions. We conduct an empirical study of the proposed framework using real-world trajectory data. It shows that the proposed framework achieves a significant improvement over the conventional ensemble approach.},
author = {Ando, Shin and Thanomphongphan, Theerasak and Seki, Yoichi and Suzuki, Einoshin},
doi = {10.1007/s10618-013-0334-x},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ando et al. - 2013 - Ensemble anomaly detection from multi-resolution trajectory features.pdf:pdf},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {Behavioral data mining,Ensemble anomaly detection,Multi-resolution features,Trajectory data mining},
month = {jan},
number = {1},
pages = {39--83},
publisher = {Springer US},
title = {{Ensemble anomaly detection from multi-resolution trajectory features}},
url = {http://link.springer.com/10.1007/s10618-013-0334-x},
volume = {29},
year = {2013}
}
@inbook{Oshiro2012,
abstract = {Random Forest is a computationally efficient technique that can operate quickly over large datasets. It has been used in many recent research projects and real-world applications in diverse domains. However, the associated literature provides almost no directions about how many trees should be used to compose a Random Forest. The research reported here analyzes whether there is an optimal number of trees within a Random Forest, i.e., a threshold from which increasing the number of trees would bring no significant performance gain, and would only increase the computational cost. Our main conclusions are: as the number of trees grows, it does not always mean the performance of the forest is significantly better than previous forests (fewer trees), and doubling the number of trees is worthless. It is also possible to state there is a threshold beyond which there is no significant gain, unless a huge computational environment is available. In addition, it was found an experimental relationship for the AUC gain when doubling the number of trees in any forest. Furthermore, as the number of trees grows, the full set of attributes tend to be used within a Random Forest, which may not be interesting in the biomedical domain. Additionally, datasets' density-based metrics proposed here probably capture some aspects of the VC dimension on decision trees and low-density datasets may require large capacity machines whilst the opposite also seems to be true.},
address = {Berlin, Heidelberg},
author = {Oshiro, Thais Mayumi and Perez, Pedro Santoro and Baranauskas, Jos{\'{e}} Augusto},
booktitle = {Machine Learning and Data Mining in Pattern Recognition: 8th International Conference, MLDM 2012, Berlin, Germany, July 13-20, 2012. Proceedings},
doi = {10.1007/978-3-642-31537-4_13},
editor = {Perner, Petra},
isbn = {978-3-642-31537-4},
pages = {154--168},
publisher = {Springer Berlin Heidelberg},
title = {{How Many Trees in a Random Forest?}},
url = {https://doi.org/10.1007/978-3-642-31537-4{\_}13},
year = {2012}
}
@article{Gama2000,
abstract = {Using multiple classifiers for increasing learning accuracy is an active research area. In this paper we present two related methods for merging classifiers. The first method, Cascade Generalization, couples classifiers loosely. It belongs to the family of stacking algorithms. The basic idea of Cascade Generalization is to use sequentially the set of classifiers, at each step performing an extension of the original data by the insertion of new attributes. The new attributes are derived from the probability class distribution given by a base classifier. This constructive step extends the representational language for the high level classifiers, relaxing their bias. The second method exploits tight coupling of classifiers, by applying Cascade Generalization locally. At each iteration of a divide and conquer algorithm, a reconstruction of the instance space occurs by the addition of new attributes. Each new attribute represents the probability that an example belongs to a class given by a base classifier. We have implemented three Local Generalization Algorithms. The first merges a linear discriminant with a decision tree, the second merges a naive Bayes with a decision tree, and the third merges a linear discriminant and a naive Bayes with a decision tree. All the algorithms show an increase of performance, when compared with the corresponding single models. Cascade also outperforms other methods for combining classifiers, like Stacked Generalization, and competes well against Boosting at statistically significant confidence levels.},
author = {Gama, Jo{\~{a}}o and Brazdil, Pavel},
doi = {10.1023/A:1007652114878},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ao Gama, Brazdil - Unknown - Cascade Generalization.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Constructive Induction,Merging Classiiers,Multiple Models},
number = {3},
pages = {315--343},
publisher = {Kluwer Academic Publishers},
title = {{Cascade Generalization}},
url = {http://www.ncc.up.pt/liacc/ML},
volume = {41},
year = {2000}
}
@article{Rayana,
abstract = {Ensemble methods for classification and clustering have been effectively used for decades, while ensemble learning for outlier detection has only been studied recently. In this work, we design a new ensemble approach for outlier detection in multi-dimensional point data, which provides improved accuracy by reducing error through both bias and variance. Although classification and outlier detection appear as different problems, their theoretical underpinnings are quite similar in terms of the bias-variance trade-off [1], where outlier detection is considered as a binary classification task with unobserved labels but a similar bias-variance decomposition of error. In this paper, we propose a sequential ensemble approach called CARE that employs a two-phase aggregation of the intermediate results in each iteration to reach the final outcome. Unlike existing outlier ensembles which solely incorporate a parallel framework by aggregating the outcomes of independent base detectors to reduce variance, our ensemble incorporates both the parallel and sequential building blocks to reduce bias as well as variance by ({\$}i{\$}) successively eliminating outliers from the original dataset to build a better data model on which outlierness is estimated (sequentially), and ({\$}ii{\$}) combining the results from individual base detectors and across iterations (parallelly). Through extensive experiments on sixteen real-world datasets mainly from the UCI machine learning repository [2], we show that CARE performs significantly better than or at least similar to the individual baselines. We also compare CARE with the state-of-the-art outlier ensembles where it also provides significant improvement when it is the winner and remains close otherwise.},
archivePrefix = {arXiv},
arxivId = {1609.05528},
author = {Rayana, Shebuti and Zhong, Wen and Akoglu, Leman},
doi = {10.1109/ICDM.2016.117},
eprint = {1609.05528},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rayana, Zhong, Akoglu - 2016 - Sequential Ensemble Learning for Outlier Detection A Bias-Variance Perspective.pdf:pdf},
pages = {11},
title = {{Sequential Ensemble Learning for Outlier Detection: A Bias-Variance Perspective}},
url = {http://arxiv.org/abs/1609.05528},
year = {2016}
}
@article{Sagha2013,
abstract = {Detection of anomalies is a broad field of study, which is applied in different areas such as data monitoring, navigation, and pattern recognition. In this paper we propose two measures to detect anomalous behaviors in an ensemble of classifiers by monitoring their decisions; one based on Mahalanobis distance and another based on information theory. These approaches are useful when an ensemble of classifiers is used and a decision is made by ordinary classifier fusion methods, while each classifier is devoted to monitor part of the environment. Upon detection of anomalous classifiers we propose a strategy that attempts to minimize adverse effects of faulty classifiers by excluding them from the ensemble. We applied this method to an artificial dataset and sensor-based human activity datasets, with different sensor configurations and two types of noise (additive and rotational on inertial sensors). We compared our method with two other well-known approaches, generalized likelihood ratio (GLR) and One-Class Support Vector Machine (OCSVM), which detect anomalies at data/feature level. We found that our method is comparable with GLR and OCSVM. The advantages of our method compared to them is that it avoids monitoring raw data or features and only takes into account the decisions that are made by their classifiers, therefore it is independent of sensor modality and nature of anomaly. On the other hand, we found that OCSVM is very sensitive to the chosen parameters and furthermore in different types of anomalies it may react differently. In this paper we discuss the application domains which benefit from our method. ?? 2013 Elsevier B.V. All rights reserved.},
author = {Sagha, Hesam and Bayati, Hamidreza and Mill{\'{a}}n, Jos{\'{e}} Del R and Chavarriaga, Ricardo},
doi = {10.1016/j.patrec.2013.02.014},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sagha et al. - 2013 - On-line anomaly detection and resilience in classifier ensembles(2).pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Anomaly detection,Classifier ensemble,Decision fusion,Human activity recognition},
number = {15},
pages = {1916--1927},
title = {{On-line anomaly detection and resilience in classifier ensembles}},
volume = {34},
year = {2013}
}
@article{Syarif,
abstract = {This paper investigates the possibility of using ensemble algorithms to improve the performance of network intrusion detection systems. We use an ensemble of three different methods, bagging, boosting and stacking, in order to improve the accuracy and reduce the false positive rate. We use four different data mining algorithms, na{\"{i}}ve bayes, J48 (decision tree), JRip (rule induction) and iBK(nearest neighbour), as base classifiers for those ensemble methods. Our experiment shows that the prototype which implements four base classifi-ers and three ensemble algorithms achieves an accuracy of more than 99{\%} in detecting known intrusions, but failed to detect novel intrusions with the accu-racy rates of around just 60{\%}. The use of bagging, boosting and stacking is un-able to significantly improve the accuracy. Stacking is the only method that was able to reduce the false positive rate by a significantly high amount (46.84{\%}); unfortunately, this method has the longest execution time and so is insufficient to implement in the intrusion detection field.},
author = {Syarif, Iwan and Zaluska, Ed and Prugel-Bennett, Adam and Wills, Gary},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Syarif et al. - Unknown - Application of Bagging, Boosting and Stacking to Intrusion Detection.pdf:pdf},
keywords = {Intrusion Detection System,bagging,boosting,ensemble classifiers,stacking},
title = {{Application of Bagging, Boosting and Stacking to Intrusion Detection}}
}
@article{Chiang2017a,
author = {Chiang, Alvin and David, Esther and Lee, Yuh-Jye and Leshem, Guy and Yeh, Yi-Ren},
doi = {10.1016/j.jal.2016.12.002},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/10.1016@j.jal.2016.12.002.pdf:pdf},
issn = {15708683},
journal = {Journal of Applied Logic},
month = {may},
pages = {1--13},
title = {{A study on anomaly detection ensembles}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1570868316301240},
volume = {21},
year = {2017}
}
@article{Sc,
address = {Cambridge, MA, USA},
author = {Sch{\"{o}}lkopf, Bernhard and Platt, John C and Shawe-Taylor, John C and Smola, Alex J and Williamson, Robert C},
doi = {10.1162/089976601750264965},
issn = {0899-7667},
journal = {Neural Comput.},
month = {jul},
number = {7},
pages = {1443--1471},
publisher = {MIT Press},
title = {{Estimating the Support of a High-Dimensional Distribution}},
url = {https://doi.org/10.1162/089976601750264965},
volume = {13},
year = {2001}
}
@inproceedings{Shoemaker:2011:ADU:2040895.2040900,
address = {Berlin, Heidelberg},
author = {Shoemaker, Larry and Hall, Lawrence O},
booktitle = {Proceedings of the 10th International Conference on Multiple Classifier Systems},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shoemaker, Hall - 2011 - Anomaly Detection Using Ensembles(2).pdf:pdf},
isbn = {978-3-642-21556-8},
keywords = {ROC curves,anomalies,data partitioning,outliers,random forests},
pages = {6--15},
publisher = {Springer-Verlag},
series = {MCS'11},
title = {{Anomaly Detection Using Ensembles}},
url = {http://dl.acm.org/citation.cfm?id=2040895.2040900},
year = {2011}
}
@inproceedings{Kozik2015,
author = {Kozik, Rafal and Choras, Michal},
booktitle = {2015 10th International Conference on P2P, Parallel, Grid, Cloud and Internet Computing (3PGCIC)},
doi = {10.1109/3PGCIC.2015.88},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/kozik2015.pdf:pdf},
isbn = {978-1-4673-9473-4},
month = {nov},
pages = {724--729},
publisher = {IEEE},
title = {{Adapting an Ensemble of One-Class Classifiers for a Web-Layer Anomaly Detection System}},
url = {http://ieeexplore.ieee.org/document/7424657/},
year = {2015}
}
@article{Hautamaki2004,
abstract = {We present an Outlier Detection using Indegree Number (ODIN) algorithm that utilizes k-nearest neighbour graph. Improvements to existing kNN distance -based method are also proposed. We compare the methods with real and syn-thetic datasets. The results show that the proposed method achieves resonable results with synthetic data and outper-forms compared methods with real data sets with small number of observations.},
author = {Hautam{\"{a}}ki, Ville and K{\"{a}}rkk{\"{a}}inen, Ismo and Fr{\"{a}}nti, Pasi},
doi = {10.1109/ICPR.2004.671},
isbn = {0-7695-2128-2},
journal = {Proceedings of the Pattern Recognition, 17th International Conference on (ICPR'04) Volume 3 - Volume 03},
pages = {430--433},
publisher = {IEEE Computer Society},
title = {{Outlier Detection Using k-Nearest Neighbour Graph}},
year = {2004}
}
@article{Tan2005,
abstract = {Cluster analysis divides data into groups (clusters) that aremeaningful, useful, or both. If meaningful groups are the goal, then the clusters should capture the natural structure of the data. In some cases, however, cluster analysis is only a useful starting point for other purposes, such as data summarization. Whether for understanding or utility, cluster analysis has long played an important role in a wide variety of fields: psychology and other social sciences, biology, statistics, pattern recognition, information retrieval, machine learning, and data mining. There have been many applications of cluster analysis to practical prob- lems. We provide some specific examples, organized by whether the purpose of the clustering is understanding or utility.},
author = {Tan, Pang-Ning and Steinbach, Michael and Kumar, Vipin},
doi = {10.1016/0022-4405(81)90007-8},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/ch8.pdf:pdf},
isbn = {0321321367},
issn = {00224405},
journal = {Introduction to Data Mining},
pages = {Chapter 8},
pmid = {21635741},
title = {{Chap 8 : Cluster Analysis: Basic Concepts and Algorithms}},
year = {2005}
}
@article{Wolpert,
abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-vali-dation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular ques-tion. After introducing stacked generalization and justifying its use, this paper presents two numer-ical experiments. The first demonstrates how stacked generalization improves upon a set of sepa-rate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other ex-perimental evidence in the literature, the usual arguments supporting cross-validation, and the ab-stract justifications presented in this paper, the conclusion is that for almost any real-world gener-alization problem one should use some version of stacked generalization to minimize the general-ization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.},
author = {Wolpert, David H},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/Wolpert1992.pdf:pdf},
keywords = {combining generalizers,cross-validation,error estimation and correction,generalization and induction,learning set pre-processing},
title = {{STACKED GENERALIZATION}}
}
@incollection{David2014,
author = {David, Esther and Leshem, Guy and Chalamish, Michal and Chiang, Alvin},
booktitle = {Technologies and Applications of Artificial Intelligence: 19th International Conference, TAAI 2014, Taipei, Taiwan, November 21-23, 2014. Proceedings},
doi = {10.1007/978-3-319-13987-6_11},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/David et al. - 2014 - Expert-Based Fusion Algorithm of an Ensemble of Anomaly Detection Algorithms.pdf:pdf},
issn = {16113349},
pages = {114--123},
publisher = {Springer, Cham},
title = {{Expert-Based Fusion Algorithm of an Ensemble of Anomaly Detection Algorithms}},
url = {http://link.springer.com/10.1007/978-3-319-13987-6{\_}11 http://link.springer.com/chapter/10.1007/978-3-319-13987-6{\_}11},
year = {2014}
}
@inproceedings{Yuan2016,
address = {New York, New York, USA},
author = {Yuan, Yali and Kaklamanos, Georgios and Hogrefe, Dieter},
booktitle = {Proceedings of the 19th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
doi = {10.1145/2988287.2989177},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/10.1145@2988287.2989177.pdf:pdf},
isbn = {978-1-4503-4502-6},
keywords = {adaboost algorithms,detection rate,execution time,false alarm rate,network anomaly detection,tri-training approach},
pages = {111--114},
publisher = {ACM Press},
title = {{A Novel Semi-Supervised Adaboost Technique for Network Anomaly Detection}},
url = {http://doi.acm.org/10.1145/2988287.2989177},
year = {2016}
}
@article{Noto2010,
abstract = {We present a new approach to semi-supervised anomaly detection. Given a set of training examples believed to come from the same distribution or class, the task is to learn a model that will be able to distinguish examples in the future that do not belong to the same class. Traditional approaches typically compare the position of a new data point to the set of "normal" training data points in a chosen representation of the feature space. For some data sets, the normal data may not have discernible positions in feature space, but do have consistent relationships among some features that fail to appear in the anomalous examples. Our approach learns to predict the values of training set features from the values of other features. After we have formed an ensemble of predictors, we apply this ensemble to new data points. To combine the contribution of each predictor in our ensemble, we have developed a novel, information-theoretic anomaly measure that our experimental results show selects against noisy and irrelevant features. Our results on 47 data sets show that for most data sets, this approach significantly improves performance over current state-of-the-art feature space distance and density-based approaches.},
author = {Noto, Keith and Brodley, Carla and Slonim, Donna},
doi = {10.1109/ICDM.2010.140},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/noto2010.pdf:pdf},
isbn = {978-1-4244-9131-5},
issn = {1550-4786},
journal = {Proceedings / IEEE International Conference on Data Mining},
keywords = {-anomaly detection,feature se-,machine learning},
pages = {953--958},
pmid = {22020249},
title = {{Anomaly Detection Using an Ensemble of Feature Models.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3197694{\&}tool=pmcentrez{\&}rendertype=abstract},
year = {2010}
}
@book{shewhart1931economic,
author = {Shewhart, Walter Andrew},
publisher = {ASQ Quality Press},
title = {{Economic control of quality of manufactured product}},
year = {1931}
}
@article{Bandaragoda2014,
author = {Bandaragoda, Tharindu R. and Ting, Kai Ming and Albrecht, David and Liu, Fei Tony and Wells, Jonathan R.},
doi = {10.1109/ICDMW.2014.70},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/bandaragoda2014.pdf:pdf},
isbn = {978-1-4799-4274-9},
issn = {23759259},
journal = {2014 IEEE International Conference on Data Mining Workshop},
keywords = {anomaly detection,ensemble-based,nearest-neighbour},
month = {dec},
pages = {698--705},
publisher = {IEEE},
title = {{Efficient Anomaly Detection by Isolation Using Nearest Neighbour Ensemble}},
url = {http://ieeexplore.ieee.org/document/7022664/ http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7022664},
year = {2014}
}
@inproceedings{Valentini:2002:ELM:645961.674073,
address = {London, UK, UK},
author = {Valentini, Giorgio and Masulli, Francesco},
booktitle = {Proceedings of the 13th Italian Workshop on Neural Nets-Revised Papers},
isbn = {3-540-44265-0},
keywords = {combining multiple learners,ensemble methods},
pages = {3--22},
publisher = {Springer-Verlag},
series = {WIRN VIETRI 2002},
title = {{Ensembles of Learning Machines}},
url = {http://dl.acm.org/citation.cfm?id=645961.674073},
year = {2002}
}
@article{atzori2010internet,
author = {Atzori, Luigi and Iera, Antonio and Morabito, Giacomo},
journal = {Computer networks},
number = {15},
pages = {2787--2805},
publisher = {Elsevier},
title = {{The internet of things: A survey}},
volume = {54},
year = {2010}
}
@article{Rayana2016,
abstract = {Ensemble learning for anomaly detection has been barely studied, due to difficulty in acquiring ground truth and the lack of inherent objective functions. In contrast, ensemble approaches for classification and clustering have been studied and effectively used for long. Our work taps into this gap and builds a new ensemble approach for anomaly detection, with application to event detection in temporal graphs as well as outlier detection in no-graph settings. It handles and combines multiple heterogeneous detectors to yield improved and robust performance. Importantly, trusting results from all the constituent detectors may deteriorate the overall performance of the ensemble, as some detectors could provide inaccurate results depending on the type of data in hand and the underlying assumptions of a detector. This suggests that combining the detectors selectively is key to building effective anomaly ensembles—hence “less is more”. In this paper we propose a novel ensemble approach called SELECT for anomaly detection, which automatically and systematically selects the results from constituent detectors to combine in a fully unsupervised fashion. We apply our method to event detection in temporal graphs and outlier detection in multi-dimensional point data (no-graph), where SELECT successfully utilizes five base detectors and seven consensus methods under a unified ensemble framework. We provide extensive quantitative evaluation of our approach for event detection on five real-world datasets (four with ground truth events), including Enron email communications, RealityMining SMS and phone call records, New York Times news corpus, and World Cup 2014 Twitter news feed. We also provide results for outlier detection on seven real-world multi-dimensional point datasets from UCI Machine Learning Repository. Thanks to its selection mechanism, SELECT yields superior performance compared to the individual detectors alone, the full ensemble (naively combining all results), an existing diversity-based ensemble, and an existing weighted ensemble approach.},
archivePrefix = {arXiv},
arxivId = {1501.01924},
author = {Rayana, Shebuti and Leman, Akoglu},
doi = {10.1145/2890508},
eprint = {1501.01924},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/1501.01924.pdf:pdf},
isbn = {9781510811522},
issn = {15564681},
journal = {ACM Transactions on Knowledge Discovery from Data},
keywords = {Ensemble methods,anomaly ensembles,anomaly mining,dynamic graphs,event detection,rank aggregation,unsupervised learning},
month = {may},
number = {4},
pages = {622--630},
publisher = {ACM},
title = {{Less is More: Building Selective Anomaly Ensembles}},
url = {http://dl.acm.org/citation.cfm?doid=2936311.2890508 dl.acm.org/citation.cfm?doid=2936311.2890508},
volume = {10},
year = {2016}
}
@article{PapadimitriouS.KitagawaH.2003,
abstract = {Outlier detection is an integral part of data mining and has attracted much attention recently [8, 15, 19]. In this paper, we propose a new method for evaluating outlier-ness, which we call the Local Correlation Integral (LOCI). As with the best previous methods, LOCI is highly effective for detecting outliers and groups of outliers (a.k.a. micro-clusters). In addition, it offers the following advantages and novelties: (a) It provides an automatic, data-dictated cut-off to determine whether a point is an outlier—in contrast, previous methods force users to pick cut-offs, without any hints as to what cut-off value is best for a given dataset. (b) It can provide a LOCI plot for each point; this plot summarizes a wealth of information about the data in the vicinity of the point, determining clusters, micro-clusters, their diameters and their inter-cluster distances. None of the existing outlier-detection methods can match this fea-ture, because they output only a single number for each point: its outlier-ness score. (c) Our LOCI method can be computed as quickly as the best previous methods. (d) Moreover, LOCI leads to a practically linear approximate method, aLOCI (for approximate LOCI), which provides fast highly-accurate outlier detection. To the best of our knowledge, this is the first work to use approximate compu-tations to speed up outlier detection. Experiments on synthetic and real world data sets show that LOCI and aLOCI can automatically detect outliers and micro-clusters, without user-required cut-offs, and that they quickly spot both expected and unexpected outliers.},
author = {{Papadimitriou, Spiros Kitagawa}, Hiroyuki and Gibbons, Phillip B and Faloutsos, Christos},
file = {::},
journal = {Proceedings of the ICDE03},
keywords = {box counting,correlation integral,outlier},
title = {{LOCI: Fast outlier detection using the local correlation integal}},
year = {2003}
}
@inbook{Malerba1996,
address = {New York, NY},
author = {Malerba, Donato and Esposito, Floriana and Semeraro, Giovanni},
booktitle = {Learning from Data: Artificial Intelligence and Statistics V},
doi = {10.1007/978-1-4612-2404-4_35},
editor = {Fisher, Doug and Lenz, Hans-J.},
isbn = {978-1-4612-2404-4},
pages = {365--374},
publisher = {Springer New York},
title = {{A Further Comparison of Simplification Methods for Decision-Tree Induction}},
url = {http://dx.doi.org/10.1007/978-1-4612-2404-4{\_}35},
year = {1996}
}
@article{Micenkova,
abstract = {Years of research in unsupervised outlier detection have pro- duced numerous algorithms to score data according to their exceptionality. However, the nature of outliers heavily de- pends on the application context and different algorithms are sensitive to outliers of different nature. This makes it very difficult to assess suitability of a particular algorithm without a priori knowledge. On the other hand, inmany ap- plications, some examples of outliers exist or can be obtained in addition to the vast amount of unlabeled data. Unfortu- nately, this extra knowledge cannot be simply incorporated into the existing unsupervised algorithms. In this paper, we show how to use powerful machine learn- ing approaches to combine labeled examples together with arbitrary unsupervised outlier scoring algorithms. We aim to get the best out of the two worlds—supervised and un- supervised. Our approach is also a viable solution to the recent problem of outlier ensemble selection. Keywords},
author = {Micenkov{\'{a}}, Barbora and McWilliams, Brian and Assent, Ira},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Micenkov{\'{a}}, Mcwilliams, Assent - Unknown - Learning Outlier Ensembles The Best of Both Worlds – Supervised and Unsupervised.pdf:pdf},
isbn = {9781450329989},
journal = {Proc. of the ACM SIGKDD Workshop on Outlier Detection and Description, ODD.},
keywords = {detection,feature construction,outlier detection,outlier ensembles,semi-supervised outlier},
pages = {1--4},
title = {{Learning Outlier Ensembles : The Best of Both Worlds – Supervised and Unsupervised}},
year = {2014}
}
@article{rumelhart1988learning,
author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J and Others},
journal = {Cognitive modeling},
number = {3},
pages = {1},
title = {{Learning representations by back-propagating errors}},
volume = {5},
year = {1988}
}
@article{Zenko2004,
abstract = {We empirically evaluate several state-of-the- art methods for constructing ensembles of heterogeneous classifiers with stacking and show that they perform (at best) compara- bly to selecting the best classifier from the ensemble by cross validation. We then pro- pose a new method for stacking, that uses multi-response model trees at the meta-level, and show that it clearly outperforms existing stacking approaches and selecting the best classifier by cross validation.},
author = {Zenko, Bernard},
doi = {10.1023/B:MACH.0000015881.36452.6e},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeroski et al. - 2004 - Is Combining Classifiers with Stacking Better than Selecting the Best One.pdf:pdf},
isbn = {1-55860-873-7},
issn = {08856125},
journal = {Machine Learning},
keywords = {combining classifiers,ensembles of classifiers,meta-learning,multi-response model trees,stacking},
number = {3},
pages = {255--273},
title = {{Is Combining Classifiers Better than Selecting the Best One?}},
volume = {54},
year = {2004}
}
@inbook{Cerqueira2016,
address = {Cham},
author = {Cerqueira, V{\'{i}}tor and Pinto, F{\'{a}}bio and S{\'{a}}, Claudio and Soares, Carlos},
booktitle = {Advances in Intelligent Data Analysis XV: 15th International Symposium, IDA 2016, Stockholm, Sweden, October 13-15, 2016, Proceedings},
doi = {10.1007/978-3-319-46349-0_35},
editor = {Bostr{\"{o}}m, Henrik and Knobbe, Arno and Soares, Carlos and Papapetrou, Panagiotis},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cerqueira et al. - 2016 - Combining Boosted Trees with Metafeature Engineering for Predictive Maintenance.pdf:pdf},
isbn = {978-3-319-46349-0},
pages = {393--397},
publisher = {Springer International Publishing},
title = {{Combining Boosted Trees with Metafeature Engineering for Predictive Maintenance}},
url = {http://dx.doi.org/10.1007/978-3-319-46349-0{\_}35},
year = {2016}
}
@article{Sewell2008,
abstract = {In biomedical data, the imbalanced data problem occurs frequently and causes poor prediction performance for minority classes. It is because the trained classifiers are mostly derived from the majority class. In this paper, we describe an ensemble learning method combined with active example selection to resolve the imbalanced data problem. Our method consists of three key components: 1) an active example selection algorithm to choose informative examples for training the classifier, 2) an ensemble learning method to combine variations of classifiers derived by active example selection, and 3) an incremental learning scheme to speed up the iterative training procedure for active example selection. We evaluate the method on six real-world imbalanced data sets in biomedical domains, showing that the proposed method outperforms both the random under sampling and the ensemble with under sampling methods. Compared to other approaches to solving the imbalanced data problem, our method excels by 0.03-0.15 points in AUC measure.},
author = {Sewell, Martin},
doi = {10.1007/978-0-387-30164-8_252},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Esposito - 2000 - Ensemble Learning.pdf:pdf},
isbn = {978-0262511025},
issn = {1941-6016},
journal = {IEEEACM transactions on computational biology and bioinformatics IEEE ACM},
number = {April 2007},
pages = {1--16},
pmid = {20876935},
title = {{Ensemble Learning}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20876935},
volume = {2007},
year = {2008}
}
@inproceedings{Liu2011,
abstract = {Anomaly detection is one of the most important applications for hyperspectral images. In this paper, a new ensemble learning algorithm for anomaly detection in hyperspectral imagery is proposed, which integrates feature grouping and anomalous signal subspace estimation. Main contribution of the proposed algorithm consists in two aspects. First, feature grouping in original hyperspectral images are firstly performed to form feature subsets with more diversity. In the subsets, conventional RX detector can better learn its model parameters. Second, an iterative orthogonal projection processing is given to estimate rare signal subspace for anomalous targets in each feature subset so as to more effectively remove background clutters. Finally, the RX detection is carried out with the estimated signal subspace in the subsets, and the detection results are combined by majority voting. Numerical experiments are conducted on real hyperspectral images and the experimental results show that the proposed algorithm outperforms several existing algorithms.},
author = {Liu, Zhenlin and Gu, Yanfeng and Wang, Chen and Han, Jinglong and Zhang, Ye},
booktitle = {Proceedings of the 2011 6th IEEE Conference on Industrial Electronics and Applications, ICIEA 2011},
doi = {10.1109/ICIEA.2011.5975970},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/liu2011.pdf:pdf},
isbn = {9781424487554},
issn = {pending},
keywords = {Hyperspectral,anomaly detection,ensemble learning,feature clustering},
pages = {2274--2277},
publisher = {IEEE},
title = {{A feature-clustering-based subspace ensemble method for anomaly detection in hyperspectral imagety}},
url = {http://ieeexplore.ieee.org/document/5975970/},
year = {2011}
}
@article{Dietterich1990,
abstract = {Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.},
author = {Dietterich, Thomas G},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dietterich - 1990 - Ensemble Methods in Machine Learning.pdf:pdf},
journal = {First International Workshop on Multiple Classifier Systems},
pages = {1--15},
title = {{Ensemble Methods in Machine Learning}},
volume = {1857},
year = {1990}
}
@article{Strehl:2003:CEK:944919.944935,
author = {Strehl, Alexander and Ghosh, Joydeep},
doi = {10.1162/153244303321897735},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
keywords = {cluster analysis,clustering,consensus functions,ensemble,knowledge reuse,multi-learner systems,mutual information,partitioning,unsupervised learning},
month = {mar},
pages = {583--617},
publisher = {JMLR.org},
title = {{Cluster Ensembles --- a Knowledge Reuse Framework for Combining Multiple Partitions}},
url = {http://dx.doi.org/10.1162/153244303321897735},
volume = {3},
year = {2003}
}
@article{Tenenboim-Chekina2011,
abstract = {Along with recent technological advances more and more new threats and advanced cyber-attacks appear unexpectedly. Developing methods which allow for identification and defense against such unknown threats is of great importance. In this paper we propose new ensemble method (which improves over the known cross-feature analysis, CFA, technique) allowing solving anom-aly detection problem in semi-supervised settings using well established super-vised learning algorithms. Theoretical correctness of the proposed method is demonstrated. Empirical evaluation results on Android malware datasets demonstrate effectiveness of the proposed approach and its superiority against the original CFA detection method.},
annote = {Mas afinal isto {\'{e}} chaining?},
author = {Tenenboim-Chekina, Lena and Rokach, Lior and Shapira, Brach},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tenenboim-Chekina, Rokach, Shapira - 2011 - Ensemble of Feature Chains for Anomaly Detection.pdf:pdf},
keywords = {Android,anomaly detection,ensemble methods,machine learning,malware,network monitoring,probabil-istic methods},
number = {1},
title = {{Ensemble of Feature Chains for Anomaly Detection}},
year = {2011}
}
@inbook{Frank2006,
address = {Berlin, Heidelberg},
author = {Frank, Eibe and Pfahringer, Bernhard},
booktitle = {Advances in Knowledge Discovery and Data Mining: 10th Pacific-Asia Conference, PAKDD 2006, Singapore, April 9-12, 2006. Proceedings},
doi = {10.1007/11731139_14},
editor = {Ng, Wee-Keong and Kitsuregawa, Masaru and Li, Jianzhong and Chang, Kuiyu},
isbn = {978-3-540-33207-7},
pages = {97--106},
publisher = {Springer Berlin Heidelberg},
title = {{Improving on Bagging with Input Smearing}},
url = {http://dx.doi.org/10.1007/11731139{\_}14},
year = {2006}
}
@article{Gagne2007,
abstract = {Evolutionary Learning (EL) proceeds by evolving a population of classifiers, and it most often (with some notable exceptions) returns the single best-of-run classifier. In the meanwhile, Ensemble Learning, one of the most efficient approaches in supervised Machine Learning for the last decade, proceeds by building a population of diverse classifiers.$\backslash$r$\backslash$n$\backslash$r$\backslash$nEnsemble Learning with Evolutionary Computation thus receives increasing attention. The Evolutionary Ensemble Learning (EEL) approach presented in this paper features two contributions.$\backslash$r$\backslash$nA new fitness function, inspired by co-evolution and enforcing the classifier diversity, is presented. Further, a new selection criterion based on the classification margin is proposed. This criterion is$\backslash$r$\backslash$nused to extract the classifier ensemble from the final population only (Off-EEL) or incrementally along evolution (On-EEL).$\backslash$r$\backslash$n$\backslash$r$\backslash$nExperiments on a set of benchmark problems show that Off-EEL outperforms single-hypothesis evolutionary learning and state-of-art Boosting and generates smaller classifier ensembles.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {0704.3905},
author = {Gagn{\'{e}}, Christian and Sebag, Michele and Schoenauer, Marc and Tomassini, Marco},
doi = {10.1145/1276958.1277317},
eprint = {0704.3905},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/3111983cbeb402290e09170ea355c29e5ca5.pdf:pdf},
isbn = {9781595936974},
journal = {Proceedings of the 9th annual conference on Genetic and evolutionary computation - GECCO '07},
keywords = {Learning/Statistics {\&} Optimisation},
pages = {1782--1789},
publisher = {ACM Press},
title = {{Ensemble Learning for Free with Evolutionary Algorithms ?}},
url = {http://portal.acm.org/citation.cfm?doid=1276958.1277317 http://eprints.pascal-network.org/archive/00003168/},
year = {2007}
}
@article{Breiman1996,
abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
author = {Breiman, Leo},
doi = {10.1023/A:1018054314350},
file = {::},
issn = {1573-0565},
journal = {Machine Learning},
number = {2},
pages = {123--140},
publisher = {Kluwer Academic Publishers-Plenum Publishers},
title = {{Bagging {\{}Predictors{\}}}},
volume = {24},
year = {1996}
}
@inproceedings{Abdullatif2016,
author = {Abdullatif, Amr and Rovetta, Stefano and Masulli, Francesco},
booktitle = {2016 IEEE 2nd International Forum on Research and Technologies for Society and Industry Leveraging a better tomorrow (RTSI)},
doi = {10.1109/RTSI.2016.7740573},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/07740573.pdf:pdf},
isbn = {978-1-5090-1131-5},
month = {sep},
pages = {1--6},
publisher = {IEEE},
title = {{Layered ensemble model for short-term traffic flow forecasting with outlier detection}},
url = {http://ieeexplore.ieee.org/document/7740573/},
year = {2016}
}
@article{Lemke2013,
abstract = {Metalearning attracted considerable interest in the machine learning community in the last years. Yet, some disagreement remains on what does or what does not constitute a metalearning problem and in which contexts the term is used in. This survey aims at giving an all-encompassing overview of the research directions pursued under the umbrella of metalearning, reconciling different definitions given in scientific literature, listing the choices involved when designing a metalearning system and identifying some of the future research challenges in this domain.},
author = {Lemke, Christiane and Budka, Marcin and Gabrys, Bogdan},
doi = {10.1007/s10462-013-9406-y},
file = {::},
issn = {0269-2821},
journal = {Artificial intelligence review},
keywords = {Life-long learning,Metaknowledge extraction,Metalearning},
number = {1},
pages = {117--130},
pmid = {26069389},
title = {{Metalearning: a survey of trends and technologies.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26069389 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4459543},
volume = {44},
year = {2013}
}
@inproceedings{Pevny2013,
abstract = {Many contemporary domains, e.g. network intrusion detection, fraud detection, etc., call for an anomaly detector processing a continuous stream of data. This need is driven by the high rate of their acquisition, limited resources for storing them, or privacy issues. The data can be also non-stationary requiring the detector to continuously adapt to their changes. A good detector for these domains should therefore have a low training and classification complexity, on-line training algorithm, and, of course, a good detection accuracy. This paper proposes a detector trying to meet all these criteria. The detector consists of multiple weak detectors, each implemented as a one-dimensional histogram. The one-dimensional histogram was chosen because it can be efficiently created on-line, and probability estimates can be efficiently retrieved from it. This construction gives the detector linear complexity of training with respect to the input dimension, number of samples, and number of weak detectors. Similarly, the classification complexity is linear with respect to number of weak detectors and the input dimension. The accuracy of the detector is compared to seven anomaly detectors from the prior art on the range of 36 classification problems from UCI database. Results show that despite detector's simplicity, its accuracy is competitive to that of more complex detectors with a substantially higher computational complexity.},
author = {Pevný, Tomáš},
booktitle = {Proceedings of the 2013 European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pevn{\'{y}} - Unknown - Anomaly Detection by Bagging.pdf:pdf},
keywords = {anomaly detection,ensemble methods,large data,on-line learning},
title = {{Anomaly Detection by Bagging}},
year = {2013}
}
@inproceedings{Zhao2015,
abstract = {Many anomaly detection algorithms have been proposed in recent years, including density-based and rank-based algorithms. In this paper, we propose ensemble methods to improve the performance of these individual algorithms. We evaluate approaches that use score and rank aggregation for these algorithms. We also consider sequential methods in which one detection method is followed by the other. We use several datasets to evaluate the performance of the proposed ensemble methods. Our results show that sequential methods significantly improve the ability to detect anomalous data points.},
author = {Zhao, Zhiruo and Mehrotra, Kishan G. and Mohan, Chilukuri K.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-19066-2_50},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao, Mehrotra, Mohan - Unknown - Ensemble Algorithms for Unsupervised Anomaly Detection.pdf:pdf},
isbn = {9783319190655},
issn = {16113349},
keywords = {Anomaly detection,Density-based anomaly detection,Ensemble method,Rank-based anomaly detection},
pages = {514--525},
publisher = {Springer, Cham},
title = {{Ensemble algorithms for unsupervised anomaly detection}},
url = {http://link.springer.com/10.1007/978-3-319-19066-2{\_}50},
volume = {9101},
year = {2015}
}
@article{Campos2016,
abstract = {The evaluation of unsupervised outlier detection algorithms is a constant challenge in data mining research. Little is known regarding the strengths and weak-nesses of different standard outlier detection models, and the impact of parameter G. O. Campos et al. choices for these algorithms. The scarcity of appropriate benchmark datasets with ground truth annotation is a significant impediment to the evaluation of outlier meth-ods. Even when labeled datasets are available, their suitability for the outlier detection task is typically unknown. Furthermore, the biases of commonly-used evaluation mea-sures are not fully understood. It is thus difficult to ascertain the extent to which newly-proposed outlier detection methods improve over established methods. In this paper, we perform an extensive experimental study on the performance of a represen-tative set of standard k nearest neighborhood-based methods for unsupervised outlier detection, across a wide variety of datasets prepared for this purpose. Based on the overall performance of the outlier detection methods, we provide a characterization of the datasets themselves, and discuss their suitability as outlier detection benchmark sets. We also examine the most commonly-used measures for comparing the perfor-mance of different methods, and suggest adaptations that are more suitable for the evaluation of outlier detection results.},
author = {Campos, Guilherme O. and Zimek, Arthur and Sander, J{\"{o}}rg and Campello, Ricardo J. G. B. and Micenkov{\'{a}}, Barbora and Schubert, Erich and Assent, Ira and Houle, Michael E.},
doi = {10.1007/s10618-015-0444-8},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Campos et al. - 2016 - On the evaluation of unsupervised outlier detection measures, datasets, and an empirical study(3).pdf:pdf},
isbn = {1061801504448},
issn = {1573756X},
journal = {Data Mining and Knowledge Discovery},
keywords = {Datasets,Evaluation,Measures,Unsupervised outlier detection},
month = {jul},
number = {4},
pages = {891--927},
title = {{On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study}},
url = {http://link.springer.com/10.1007/s10618-015-0444-8},
volume = {30},
year = {2016}
}
@article{Schapire2003,
abstract = {A team of classifiers (committee of learners) can be more accurate than the best member of the team. Theoretically, if the classifiers make independent errors, the majority vote outperforms the best classifier. However, if the classifiers are dependent, the team might be either better or worse. While there are many measures for dependency between two variables (here the classifier outputs), measuring the dependency or diversity of many variables is not straightforward. Here we study eight...},
author = {Schapire, Robert E},
doi = {10.1049/ic:20010105},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schapire - 2003 - Measures of Diversity in Classifier Ensembles.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {committee learners,dependency diversity,multiple classifiers ensemble,pattern recognition},
number = {2},
pages = {181--207},
title = {{Measures of Diversity in Classifier Ensembles}},
url = {http://citeseer.ist.psu.edu/kuncheva00measures.html},
volume = {51},
year = {2003}
}
@inproceedings{freund1995desicion,
author = {Freund, Yoav and Schapire, Robert E},
booktitle = {European conference on computational learning theory},
organization = {Springer},
pages = {23--37},
title = {{A desicion-theoretic generalization of on-line learning and an application to boosting}},
year = {1995}
}
@book{Aggarwal:2013:OA:2436823,
author = {Aggarwal, Charu C},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aggarwal - 2017 - Outlier Analysis.pdf:pdf},
isbn = {1461463955, 9781461463955},
publisher = {Springer Publishing Company, Incorporated},
title = {{Outlier Analysis}},
year = {2017}
}
@inproceedings{Parhizkar2015,
author = {Parhizkar, Elham and Abadi, Mahdi},
booktitle = {2015 23rd Iranian Conference on Electrical Engineering},
doi = {10.1109/IranianCEE.2015.7146291},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/07146291.pdf:pdf},
isbn = {978-1-4799-1972-7},
month = {may},
pages = {631--636},
publisher = {IEEE},
title = {{OC-WAD: A one-class classifier ensemble approach for anomaly detection in web traffic}},
url = {http://ieeexplore.ieee.org/document/7146291/},
year = {2015}
}
@article{Zimek2013,
abstract = {Outlier detection and ensemble learning are well established research directions in data mining yet the application of en-semble techniques to outlier detection has been rarely stud-ied. Here, we propose and study subsampling as a technique to induce diversity among individual outlier detectors. We show analytically and experimentally that an outlier detec-tor based on a subsample per se, besides inducing diversity, can, under certain conditions, already improve upon the re-sults of the same outlier detector on the complete dataset. Building an ensemble on top of several subsamples is further improving the results. While in the literature so far the intu-ition that ensembles improve over single outlier detectors has just been transferred from the classification literature, here we also justify analytically why ensembles are also expected to work in the unsupervised area of outlier detection. As a side effect, running an ensemble of several outlier detectors on subsamples of the dataset is more efficient than ensembles based on other means of introducing diversity and, depend-ing on the sample rate and the size of the ensemble, can be even more efficient than just the single outlier detector on the complete data.},
address = {New York, New York, USA},
author = {Zimek, Arthur and Gaudet, Matthew and Campello, Ricardo J.G.B. G B and Sander, J{\"{o}}rg},
doi = {10.1145/2487575.2487676},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/subsampling-outlier-ensemble.pdf:pdf},
isbn = {9781450321747},
issn = {19310145},
journal = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '13},
keywords = {ensemble,outlier detection,unsupervised},
pages = {428},
publisher = {ACM Press},
title = {{Subsampling for efficient and effective unsupervised outlier detection ensembles}},
url = {http://dl.acm.org/citation.cfm?doid=2487575.2487676 http://dl.acm.org/citation.cfm?id=2487575.2487676{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2487575.2487676},
year = {2013}
}
@article{Ghafoori2016,
author = {Ghafoori, Zahra and Erfani, Sarah M. and Rajasegarar, Sutharshan and Karunasekera, Shanika and Leckie, Christopher A.},
doi = {10.1109/IJCNN.2016.7727507},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/ghafoori2016.pdf:pdf},
isbn = {9781509006205},
journal = {IJCNN},
month = {jul},
pages = {2476--2483},
publisher = {IEEE},
title = {{Anomaly Detection in Non-stationary Data : Ensemble based Self-Adaptive OCSVM}},
url = {http://ieeexplore.ieee.org/document/7727507/},
year = {2016}
}
@inproceedings{Kushmerick:1999:LRI:301136.301186,
address = {New York, NY, USA},
author = {Kushmerick, Nicholas},
booktitle = {Proceedings of the Third Annual Conference on Autonomous Agents},
doi = {10.1145/301136.301186},
isbn = {1-58113-066-X},
pages = {175--181},
publisher = {ACM},
series = {AGENTS '99},
title = {{Learning to Remove Internet Advertisements}},
url = {http://doi.acm.org/10.1145/301136.301186},
year = {1999}
}
@article{Hodge,
abstract = {Outlier detection has been used for centuries to detect and, where appropriate, remove anomalous observations from data. Outliers arise due to mechanical faults, changes in system behaviour, fraudulent behaviour, human error, instrument error or simply through natural deviations in populations. Their detection can identify system faults and fraud before they escalate with potentially catastrophic consequences. It can identify errors and remove their contaminating effect on the data set and as such to purify the data for processing. The original outlier detection methods were arbitrary but now, principled and systematic techniques are used, drawn from the full gamut of Computer Science and Statistics. In this paper, we introduce a survey of contemporary techniques for outlier detection. We identify their respective motivations and distinguish their advantages and disadvantages in a comparative review.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hodge, Victoria J and Austin, Jim},
doi = {10.1007/s10462-004-4304-y},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hodge, Austin - Unknown - A Survey of Outlier Detection Methodologies.pdf:pdf},
isbn = {1856044637},
issn = {0269-2821, 1573-7462},
journal = {Artificial Intelligence Review},
keywords = {anomaly,detection,deviation,noise,novelty,outlier,recognition},
number = {1969},
pages = {85--126},
pmid = {24579930},
title = {{A Survey of Outlier Detection Methodoligies}},
url = {http://link.springer.com/article/10.1007/s10462-004-4304-y},
volume = {22},
year = {2004}
}
@article{Reddy2014,
author = {Reddy, R. Ravinder},
doi = {10.1109/ICISA.2014.6847454},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/06847454.pdf:pdf},
isbn = {9781479944415},
journal = {2014 International Conference on Information Science {\&} Applications (ICISA)},
month = {may},
pages = {5--8},
publisher = {IEEE},
title = {{Real time anomaly detection using Ensembles}},
url = {http://ieeexplore.ieee.org/document/6847454/},
year = {2014}
}
@article{ting1997stacked,
author = {Ting, Kai Ming and Witten, Ian H},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/97KMT-IHW-Stacked.pdf:pdf},
publisher = {Department of Computer Science, University of Waik},
title = {{Stacked Generalization: when does it work?}},
year = {1997}
}
@inproceedings{Chiang2015,
author = {Chiang, Alvin and Yeh, Yi-Ren},
booktitle = {2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)},
doi = {10.1109/WI-IAT.2015.260},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chiang, Yeh - 2015 - Anomaly Detection Ensembles In Defense of the Average(2).pdf:pdf},
isbn = {978-1-4673-9618-9},
month = {dec},
pages = {207--210},
publisher = {IEEE},
title = {{Anomaly Detection Ensembles: In Defense of the Average}},
url = {http://ieeexplore.ieee.org/document/7397458/},
year = {2015}
}
@inproceedings{Gao:2006:COS:1193207.1193286,
address = {Washington, DC, USA},
author = {Gao, Jing and Tan, Pang-Ning},
booktitle = {Proceedings of the Sixth International Conference on Data Mining},
doi = {10.1109/ICDM.2006.43},
isbn = {0-7695-2701-9},
pages = {212--221},
publisher = {IEEE Computer Society},
series = {ICDM '06},
title = {{Converting Output Scores from Outlier Detection Algorithms into Probability Estimates}},
url = {http://dx.doi.org/10.1109/ICDM.2006.43},
year = {2006}
}
@article{Langone2015,
abstract = {Accurate prediction of forthcoming faults in modern industrial machines plays a key role in reducing production arrest, increasing the safety of plant operations, and optimizing manufacturing costs. The most effective condition monitoring techniques are based on the analysis of historical process data. In this paper we show how Least Squares Support Vector Machines (LS-SVMs) can be used effectively for early fault detection in an online fashion. Although LS-SVMs are existing artificial intelligence methods, in this paper the novelty is represented by their successful application to a complex industrial use case, where other approaches are commonly used in practice. In particular, in the first part we present an unsupervised approach that uses Kernel Spectral Clustering (KSC) on the sensor data coming from a vertical form seal and fill (VFFS) machine, in order to distinguish between normal operating condition and abnormal situations. Basically, we describe how KSC is able to detect in advance the need of maintenance actions in the analysed machine, due the degradation of the sealing jaws. In the second part we illustrate a nonlinear auto-regressive (NAR) model, thus a supervised learning technique, in the LS-SVM framework. We show that we succeed in modelling appropriately the degradation process affecting the machine, and we are capable to accurately predict the evolution of dirt accumulation in the sealing jaws.},
author = {Langone, Rocco and Alzate, Carlos and {De Ketelaere}, Bart and Vlasselaer, Jonas and Meert, Wannes and Suykens, Johan A K},
doi = {10.1016/j.engappai.2014.09.008},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/Langone{\_}maintenanceMachines.pdf:pdf},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Artificial intelligence,Fault detection,Kernel spectral clustering,LS-SVMs,Machine degradation,Time-series prediction},
pages = {268--278},
publisher = {Elsevier},
title = {{LS-SVM based spectral clustering and regression for predicting maintenance of industrial machines}},
volume = {37},
year = {2015}
}
@article{ho1998random,
author = {Ho, Tin Kam},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {8},
pages = {832--844},
publisher = {IEEE},
title = {{The random subspace method for constructing decision forests}},
volume = {20},
year = {1998}
}
@article{Kandhari2009,
abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains.Many anomaly detection techniques have been specifically developed for certain appli- cation domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection.We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key as- sumptions, which are used by the techniques to differentiate between normal and anomalous behavior.When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category.We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with},
author = {Kandhari, Rupali and Chandola, Varun and Banerjee, Arindam and Kumar, Vipin and Kandhari, Rupali},
doi = {10.1145/1541880.1541882},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kandhari et al. - 2009 - Anomaly detection.pdf:pdf},
isbn = {0818663359},
issn = {03600300},
journal = {ACM Computing Surveys},
keywords = {Anomaly detection,outlier detection},
number = {3},
pages = {1--6},
pmid = {21834704},
title = {{Anomaly detection}},
volume = {41},
year = {2009}
}
@article{hansen1990neural,
author = {Hansen, Lars Kai and Salamon, Peter},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {10},
pages = {993--1001},
publisher = {IEEE},
title = {{Neural network ensembles}},
volume = {12},
year = {1990}
}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\{}{\&}{\}} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
doi = {10.1023/A:1010933404324},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/randomforest2001.pdf:pdf},
issn = {1573-0565},
journal = {Machine Learning},
number = {1},
pages = {5--32},
title = {{Random Forests}},
url = {http://dx.doi.org/10.1023/A:1010933404324},
volume = {45},
year = {2001}
}
@book{Mendes-Moreira2012,
abstract = {The goal of ensemble regression is to combine several models in order to improve the prediction accuracy in learning problems with a numerical target variable. The process of ensemble learning can be divided into three phases: the generation phase, the pruning phase, and the integration phase.We discuss different approaches to each of these phases that are able to deal with the regression problem, categorizing them in terms of their relevant characteristics and linking them to contributions from different fields. Furthermore, this work makes it possible to identify interesting areas for future research.},
author = {Mendes-Moreira, Jo{\~{a}}o and Soares, Carlos and Jorge, Al{\'{i}}pio M{\'{a}}rio and Sousa, Jorge Freire De},
booktitle = {ACM Computing Surveys},
doi = {10.1145/2379776.2379786},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mendes-Moreira et al. - 2012 - Ensemble approaches for regression(2).pdf:pdf},
isbn = {3512250815},
issn = {03600300},
keywords = {ensembles,regression,supervised learning},
number = {1},
pages = {1--40},
publisher = {ACM},
title = {{Ensemble approaches for regression}},
volume = {45},
year = {2012}
}
@article{Curiac2012,
abstract = {Wireless sensor networks are often used to monitor and measure physical characteristics from remote and sometimes hostile environments. In these circumstances the sensing data accuracy is a crucial attribute for the way these applications complete their objectives, requiring efficient solutions to discover sensor anomalies. Such solutions are hard to be found mainly because the intricate defining of the correct sensor behavior in a complex and dynamic environment. This paper tackles the sensing anomaly detection from a new perspective by modeling the correct operation of sensors not by one, but by five different dynamical models, acting synergically to provide a reliable solution. Our methodology relies on an ensemble based system composed of a set of diverse binary classifiers, adequately selected, to implement a complex decisional system on network base station. Moreover, every time a sensing anomaly is discovered, our ensemble offers a reliable estimation to replace the erroneous measurement provided by sensor.},
author = {Curiac, Daniel-Ioan and Volosencu, Constantin},
doi = {10.1016/j.eswa.2012.02.036},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Curiac, Volosencu - 2012 - Ensemble based sensing anomaly detection in wireless sensor networks.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {wireless sensor networks},
number = {10},
pages = {9087--9096},
title = {{Ensemble based sensing anomaly detection in wireless sensor networks}},
url = {http://dx.doi.org/10.1016/j.eswa.2012.02.036},
volume = {39},
year = {2012}
}
@book{Kohonen:1997:SM:261082,
address = {Secaucus, NJ, USA},
editor = {Kohonen, Teuvo},
isbn = {3-540-62017-6},
publisher = {Springer-Verlag New York, Inc.},
title = {{Self-organizing Maps}},
year = {1997}
}
@article{1677518,
author = {Rodriguez, J J and Kuncheva, L I and Alonso, C J},
doi = {10.1109/TPAMI.2006.211},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Automated;Principal Component Analysis;Reproducib,Computer-Assisted;Pattern Recognition,Statistical;Numerical Analysis,decision trees;feature extraction;pattern classifi},
month = {oct},
number = {10},
pages = {1619--1630},
title = {{Rotation Forest: A New Classifier Ensemble Method}},
volume = {28},
year = {2006}
}
@book{Aggarwal:2015:DMT:2778285,
author = {Aggarwal, Charu C},
isbn = {978-3-319-14142-8},
publisher = {Springer Publishing Company, Incorporated},
title = {{Data Mining: The Textbook}},
year = {2015}
}
@inbook{Ruta2001,
address = {Berlin, Heidelberg},
author = {Ruta, Dymitr and Gabrys, Bogdan},
booktitle = {Multiple Classifier Systems: Second International Workshop, MCS 2001 Cambridge, UK, July 2--4, 2001 Proceedings},
doi = {10.1007/3-540-48219-9_40},
editor = {Kittler, Josef and Roli, Fabio},
isbn = {978-3-540-48219-2},
pages = {399--408},
publisher = {Springer Berlin Heidelberg},
title = {{Application of the Evolutionary Algorithms for Classifier Selection in Multiple Classifier Systems with Majority Voting}},
url = {http://dx.doi.org/10.1007/3-540-48219-9{\_}40},
year = {2001}
}
@article{bergstra2012random,
author = {Bergstra, James and Bengio, Yoshua},
journal = {Journal of Machine Learning Research},
number = {Feb},
pages = {281--305},
title = {{Random search for hyper-parameter optimization}},
volume = {13},
year = {2012}
}
@article{Aggarwal2013,
abstract = {Ensemble analysis is a widely used meta-algorithm for many data mining problems such as classification and clustering. Numerous ensemble-based algorithms have been proposed in the literature for these problems. Compared to the cluster-ing and classification problems, ensemble analysis has been studied in a limited way in the outlier detection literature. In some cases, ensemble analysis techniques have been im-plicitly used by many outlier analysis algorithms, but the approach is often buried deep into the algorithm and not for-mally recognized as a general-purpose meta-algorithm. This is in spite of the fact that this problem is rather important in the context of outlier analysis. This paper discusses the various methods which are used in the literature for outlier ensembles and the general principles by which such analysis can be made more effective. A discussion is also provided on how outlier ensembles relate to the ensemble-techniques used commonly for other data mining problems.},
author = {Aggarwal, Charu C},
doi = {10.1145/2481244.2481252},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aggarwal - Unknown - Outlier Ensembles Position Paper.pdf:pdf},
issn = {1931-0145},
journal = {ACM SIGKDD Explorations Newsletter},
number = {2},
pages = {49--58},
title = {{Outlier ensembles: position paper}},
volume = {14},
year = {2013}
}
@article{wolpert1992stacked,
author = {Wolpert, David H},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/Wolpert1992.pdf:pdf},
journal = {Neural networks},
number = {2},
pages = {241--259},
publisher = {Elsevier},
title = {{Stacked generalization}},
volume = {5},
year = {1992}
}
@incollection{Schubert2015,
abstract = {Popular outlier detection methods require the pairwise com-parison of objects to compute the nearest neighbors. This inherently quadratic problem is not scalable to large data sets, making multidi-mensional outlier detection for big data still an open challenge. Exist-ing approximate neighbor search methods are designed to preserve dis-tances as well as possible. In this article, we present a highly scalable approach to compute the nearest neighbors of objects that instead fo-cuses on preserving neighborhoods well using an ensemble of space-filling curves. We show that the method has near-linear complexity, can be dis-tributed to clusters for computation, and preserves neighborhoods—but not distances—better than established methods such as locality sensi-tive hashing and projection indexed nearest neighbors. Furthermore, we demonstrate that, by preserving neighborhoods, the quality of outlier detection based on local density estimates is not only well retained but sometimes even improved, an effect that can be explained by relating our method to outlier detection ensembles. At the same time, the outlier detection process is accelerated by two orders of magnitude.},
author = {Schubert, Erich and Zimek, Arthur and Kriegel, Hans-Peter},
booktitle = {Database Systems for Advanced Applications, Dasfaa 2015, Pt Ii},
doi = {10.1007/978-3-319-18123-3_2},
file = {::},
isbn = {978-3-319-18123-3; 978-3-319-18122-6},
pages = {19--36},
publisher = {Springer, Cham},
title = {{Fast and Scalable Outlier Detection with Approximate Nearest Neighbor Ensembles}},
url = {http://link.springer.com/10.1007/978-3-319-18123-3{\_}2 http://dx.doi.org/10.1007/978-3-319-18123-3{\_}2{\%}5Cnhttp://www.dbs.ifi.lmu.de{\%}5Cn{\%}3CGo to ISI{\%}3E://WOS:000361843500002},
volume = {9050},
year = {2015}
}
@article{leroy1987robust,
author = {Leroy, Annick M and Rousseeuw, Peter J},
journal = {Wiley Series in Probability and Mathematical Statistics, New York: Wiley, 1987},
title = {{Robust regression and outlier detection}},
year = {1987}
}
@article{Nawata2010,
abstract = {We propose an anomaly detection method that trains a baseline model describing the normal behavior of network traffic without using manually labeled traffic data. The trained baseline distribution is used as the basis for comparison with the audit network traffic. The proposed method can be carried out in an unsupervised manner through the use of time-periodical packet sampling for a different purpose from which it was intended. That is, we take advantage of the lossy nature of packet sampling for the purpose of extracting normal packets from the unlabeled original traffic data. By using real traffic traces, we show that the proposed method is comparable in terms of false positive and false negative rates on detecting anomalies regarding TCP SYN packets to the conventional method that requires manually labeled traffic data to train the baseline model. In addition, in order to mitigate the possible performance variation due to probabilistic nature of sampled traffic data, we devised an ensemble anomaly detection method that exploits multiple baseline models in parallel. Experimental results show that the proposed ensemble anomaly detection performs well and is not affected by the variability of time-periodical packet sampling.},
author = {Uchida, Masato and Nawata, Shuichi and Gu, Yu and Tsuru, Masato and Oie, Yuji},
doi = {10.1587/transcom.E95.B.2358},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/05466662.pdf:pdf},
isbn = {9781424467396},
issn = {09168516},
journal = {IEICE Transactions on Communications},
keywords = {Anomaly detection,Packet sampling},
month = {mar},
number = {7},
pages = {2358--2367},
publisher = {IEEE},
title = {{Unsupervised ensemble anomaly detection using time-periodic packet sampling}},
url = {http://ieeexplore.ieee.org/document/5466662/},
volume = {E95-B},
year = {2012}
}
@article{Bosman2015,
abstract = {In the past decade, rapid technological advances in the fields of electronics and telecommunications have given rise to versatile, ubiquitous decentralized embedded sensor systems with ad hoc wireless networking capabilities. Typically these systems are used to gather large amounts of data, while the detection of anomalies (such as system failures, intrusion, or unanticipated behavior of the environment) in the data (or other types or processing) is performed in centralized computer systems. In spite of the great interest that it attracts, the systematic porting and analysis of centralized anomaly detection algorithms to a decentralized paradigm (compatible with the aforementioned sensor systems) has not been thoroughly addressed in the literature. We approach this task from a new angle, assessing the viability of localized (in-node) anomaly detection based on machine learning. The main challenges we address are: (1) deploying decentralized, automated, online learning, anomaly detection algorithms within the stringent constraints of typical embedded systems; and (2) evaluating the performance of such algorithms and comparing them with that of centralized ones. To this end, we first analyze (and port) single and multi-dimensional input classifiers that are trained incrementally online and whose computational requirements are compatible with the limitations of embedded platforms. Next, we combine multiple classifiers in a single online ensemble. Then, using both synthetic and real-world datasets from different application domains, we extensively evaluate the anomaly detection performance of our algorithms and ensemble, in terms of precision and recall, and compare it to that of well-known offline, centralized machine learning algorithms. Our results show that the ensemble performs better than each individual decentralized classifier and that it can match the performance of the offline alternatives, thus showing that our approach is a viable solution to detect anomalies, even in environments with little a priori knowledge.},
author = {Bosman, Hedde H W J and Iacca, Giovanni and Tejada, Arturo and W{\"{o}}rtche, Heinrich J and Liotta, Antonio},
doi = {10.1016/j.adhoc.2015.07.013},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bosman et al. - 2015 - Ensembles of incremental learners to detect anomalies in ad hoc sensor networks.pdf:pdf},
isbn = {15708705 (ISSN)},
issn = {15708705},
journal = {Ad Hoc Networks},
keywords = {Anomaly detection,Ensemble methods,Incremental learning,Online learning,Wireless sensor networks},
pages = {14--36},
title = {{Ensembles of incremental learners to detect anomalies in ad hoc sensor networks}},
volume = {35},
year = {2015}
}
@inproceedings{Agrawal2015,
abstract = {In the present world huge amounts of data are stored and transferred from one location to another. The data when transferred or stored is primed exposed to attack. Although various techniques or applications are available to protect data, loopholes exist. Thus to analyze data and to determine various kind of attack data mining techniques have emerged to make it less vulnerable. Anomaly detection uses these data mining techniques to detect the surprising behaviour hidden within data increasing the chances of being intruded or attacked. Various hybrid approaches have also been made in order to detect known and unknown attacks more accurately. This paper reviews various data mining techniques for anomaly detection to provide better understanding among the existing techniques that may help interested researchers to work future in this direction.},
author = {Agrawal, Shikha and Agrawal, Jitendra},
booktitle = {Procedia Computer Science},
doi = {10.1016/j.procs.2015.08.220},
file = {:C$\backslash$:/Users/Miguel Sandim/Downloads/agrawal2015.pdf:pdf},
issn = {18770509},
keywords = {Anomaly detection,Classification,Clustering,Data mining,Intrusion detection system},
number = {1},
pages = {708--713},
title = {{Survey on anomaly detection using data mining techniques}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877050915023479},
volume = {60},
year = {2015}
}
@book{Kuncheva:2004:CPC:975251,
author = {Kuncheva, Ludmila I},
isbn = {0471210781},
publisher = {Wiley-Interscience},
title = {{Combining Pattern Classifiers: Methods and Algorithms}},
year = {2004}
}
@article{Sandim2016,
abstract = {— Automatic Vehicle Location (AVL) is becoming an important tool in Intelligent Transportation Systems (ITS) in the past few years, as it is an effective way of collecting and transmitting data regarding the vehicle's trip for real-time or future use. A methodology for analyzing the state of the art regarding the application of these systems is proposed in a form of a systematic literature review, by identifying and systematizing possible transportation network performance metrics that can be calculated or predicted using GPS-based AVL systems and inferring tendencies observed throughout the literature regarding techniques used and sensor data source and usage. As a result of this research, several performance metrics were identified, with Travel Time and Average Speed being the most recurrent ones. The conclusions reveal an increase in the number of publications and research projects regarding this topic over the years, as well as a promising potential of this type of technology, with buses and taxis being the most used probe vehicles.},
author = {Sandim, Miguel and Rossetti, Rosaldo J F and Moura, Daniel C},
file = {:C$\backslash$:/Users/Miguel Sandim/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sandim et al. - Unknown - Using GPS-based AVL Data to Calculate and Predict Traffic Network Performance Metrics a Systematic Review.pdf:pdf},
isbn = {9781509018895},
keywords = {Data Mining and Data Analysis,Intelligent Logistics,Simulation and Modeling},
pages = {1692--1699},
title = {{A Systematic Review on Using AVL Data to Obtain and Predict Traffic Network Performance Metrics}},
year = {2016}
}
